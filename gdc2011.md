# Notes from GDC 2021

This document contains notes recorded from talks focused primarily around Tools, Testing, and AI.

Table of Contents:

- [Notes from GDC 2021](#notes-from-gdc-2021)
  - [Online Game Technology Summit: Exploring Services Architecture at Bungie](#online-game-technology-summit-exploring-services-architecture-at-bungie)
    - [TL;DR:](#tldr)
    - [Summary](#summary)
    - [Raw Notes](#raw-notes)
  - [Tools Summit: The Rust Programming Language for Game Tooling](#tools-summit-the-rust-programming-language-for-game-tooling)
    - [TL;DR:](#tldr-1)
    - [Summary](#summary-1)
    - [Raw Notes](#raw-notes-1)
  - [AI Summit: Where The $@*&% Are Your Tests?!](#ai-summit-where-the--are-your-tests)
    - [TL;DR:](#tldr-2)
    - [Summary](#summary-2)
    - [Raw Notes](#raw-notes-2)
  - [Tools Summit: Embedded Scripting with Zero Overhead in Final Builds](#tools-summit-embedded-scripting-with-zero-overhead-in-final-builds)
    - [TL;DR:](#tldr-3)
    - [Summary](#summary-3)
    - [Raw Notes](#raw-notes-3)
  - [Tools Summit: LIVE Game Show](#tools-summit-live-game-show)
  - [Independent Games Summit: You Have to Stop Working, Even When You Can't Stop Working (lightning talks)](#independent-games-summit-you-have-to-stop-working-even-when-you-cant-stop-working-lightning-talks)
    - [TL;DR:](#tldr-4)
    - [Summary](#summary-4)
    - [Raw Notes](#raw-notes-4)
  - [Board Game Design Summit: Creating Immersion in Tabletop Games](#board-game-design-summit-creating-immersion-in-tabletop-games)
    - [TL;DR:](#tldr-5)
    - [Summary](#summary-5)
    - [Raw Notes](#raw-notes-5)
  - [AI Summit: Automated Game Testing Using a Numeric Domain Independent AI Planner](#ai-summit-automated-game-testing-using-a-numeric-domain-independent-ai-planner)
    - [TL;DR:](#tldr-6)
    - [Summary](#summary-6)
    - [Raw Notes](#raw-notes-6)
  - [Fair Play Summit: Tackling Cheating In Gaming](#fair-play-summit-tackling-cheating-in-gaming)
    - [TL;DR:](#tldr-7)
    - [Summary](#summary-7)
    - [Raw Notes](#raw-notes-7)
  - [AI Summit: Your Buddy, the Grandmaster: Repurposing Gameplaying AI for Inclusivity](#ai-summit-your-buddy-the-grandmaster-repurposing-gameplaying-ai-for-inclusivity)
    - [TL;DR:](#tldr-8)
    - [Summary](#summary-8)
    - [Raw Notes](#raw-notes-8)
  - [Game Narrative Summit: Turn Your Writers Into Programmers: Greyboxing Narrative with Story Languages](#game-narrative-summit-turn-your-writers-into-programmers-greyboxing-narrative-with-story-languages)
    - [TL;DR:](#tldr-9)
    - [Summary](#summary-9)
    - [Raw Notes](#raw-notes-9)
  - [Math in Game Development Summit: Even Faster Math Functions](#math-in-game-development-summit-even-faster-math-functions)
    - [TL;DR:](#tldr-10)
    - [Summary](#summary-10)
    - [Raw Notes](#raw-notes-10)
  - [Math In Game Development Summit: Mathematical Analysis of Classic Card and Board Games](#math-in-game-development-summit-mathematical-analysis-of-classic-card-and-board-games)
    - [TL;DR:](#tldr-11)
    - [Summary](#summary-11)
    - [Raw Notes](#raw-notes-11)
  - [Machine Learning Summit: ML for Art and Game Balancing with Project Chimera](#machine-learning-summit-ml-for-art-and-game-balancing-with-project-chimera)
    - [TL;DR:](#tldr-12)
    - [Summary](#summary-12)
    - [Raw Notes](#raw-notes-12)
  - [Machine Learning Summit: Smart Tech in 'Hearthstone'](#machine-learning-summit-smart-tech-in-hearthstone)
    - [TL;DR:](#tldr-13)
    - [Summary](#summary-13)
    - [Raw Notes](#raw-notes-13)
  - [Automated Testing Roundtable Day 1: Process](#automated-testing-roundtable-day-1-process)
    - [TL;DR: Communication is key to finding the correct process. Test automation requires schedule space to deliver value.](#tldr-communication-is-key-to-finding-the-correct-process-test-automation-requires-schedule-space-to-deliver-value)
    - [Summary](#summary-14)
    - [Raw Notes:](#raw-notes-14)
  - [Technical Issues in Tools Development Roundtable Day 1: Assets](#technical-issues-in-tools-development-roundtable-day-1-assets)
    - [TL;DR:](#tldr-14)
    - [Summary](#summary-15)
    - [Raw Notes](#raw-notes-15)
  - [Tools Design Roundtable Day 1: Design](#tools-design-roundtable-day-1-design)
    - [TL;DR:](#tldr-15)
    - [Summary](#summary-16)
    - [Raw Notes](#raw-notes-16)
  - [Scriptable Core Dump Debugging and Memory Stomp Tracking](#scriptable-core-dump-debugging-and-memory-stomp-tracking)
    - [TL;DR:](#tldr-16)
    - [Summary](#summary-17)
    - [Raw Notes](#raw-notes-17)
  - [An Open Source Foundation for the Game Industry Roundtable](#an-open-source-foundation-for-the-game-industry-roundtable)
    - [TL;DR:](#tldr-17)
    - [Summary](#summary-18)
    - [Raw Notes](#raw-notes-18)
  - [Automated Testing Roundtable Day 2: Legacy](#automated-testing-roundtable-day-2-legacy)
    - [TL;DR:](#tldr-18)
    - [Summary](#summary-19)
    - [Raw Notes](#raw-notes-19)
  - [Lessons Learned in Adapting the 'Sea of Thieves' Automated Testing Methodology to 'Minecraft'](#lessons-learned-in-adapting-the-sea-of-thieves-automated-testing-methodology-to-minecraft)
    - [TL;DR:](#tldr-19)
    - [Summary](#summary-20)
    - [Raw Notes](#raw-notes-20)
  - [Interviewing for Design Roundtable Day 1](#interviewing-for-design-roundtable-day-1)
    - [TL;DR:](#tldr-20)
    - [Summary](#summary-21)
    - [Raw Notes](#raw-notes-21)
  - [Technical Issues in Tools Development Roundtable Day 2: Pipeline](#technical-issues-in-tools-development-roundtable-day-2-pipeline)
    - [TL;DR:](#tldr-21)
    - [Summary](#summary-22)
    - [Raw Notes](#raw-notes-22)
  - [Final Fantasy VII Remake: Automating Quality Assurance and the Tools for the Future](#final-fantasy-vii-remake-automating-quality-assurance-and-the-tools-for-the-future)
    - [TL;DR:](#tldr-22)
    - [Summary](#summary-23)
    - [Raw Notes](#raw-notes-23)
  - [Better Live Game Releases with Persona Based Testing](#better-live-game-releases-with-persona-based-testing)
    - [TL;DR:](#tldr-23)
    - [Summary](#summary-24)
    - [Raw Notes](#raw-notes-24)
  - [Automated Testing Roundtable Day 3: Implementation](#automated-testing-roundtable-day-3-implementation)
    - [TL;DR:](#tldr-24)
    - [Summary](#summary-25)
    - [Raw Notes](#raw-notes-25)
  - [Technical Issues in Tools Development Roundtable Day 3: Workflow](#technical-issues-in-tools-development-roundtable-day-3-workflow)
    - [TL;DR:](#tldr-25)
    - [Summary](#summary-26)
    - [Raw Notes](#raw-notes-26)
  - [Using Non-Functional Testing to Guide Game Development (Presented by HeadSpin)](#using-non-functional-testing-to-guide-game-development-presented-by-headspin)
    - [TL;DR:](#tldr-26)
    - [Summary](#summary-27)
    - [Raw Notes](#raw-notes-27)
  - [Beyond Test Servers: How 'For Honor' Made Testing a Celebrated Player Experience](#beyond-test-servers-how-for-honor-made-testing-a-celebrated-player-experience)
    - [TL;DR:](#tldr-27)
    - [Summary](#summary-28)
    - [Raw Notes](#raw-notes-28)
  - [Creating a Sustainable Volunteer Game Development Community](#creating-a-sustainable-volunteer-game-development-community)
    - [TL;DR:](#tldr-28)
    - [Summary](#summary-29)
    - [Raw Notes](#raw-notes-29)
  - [Indie Soapbox](#indie-soapbox)
    - [TL;DR:](#tldr-29)
    - [Summary](#summary-30)
    - [Raw Notes](#raw-notes-30)

## Online Game Technology Summit: Exploring Services Architecture at Bungie

Michael Williams, Central Technology Engineering Lead at Bungie
https://www.gdcvault.com/play/1027046/Online-Game-Technology-Summit-Exploring
### TL;DR:
Queue throttle controls critical for backend, control service "constants" with runtime editable variables, soft launch service replacement with side-by-side A/B treatment for easy rollback.
### Summary
This was a crash course in online service design from the ground-up, outlining how Bungie's legacy implementation from the original Halo games changed to scale up to an MMO-scope in Destiny and Destiny 2. It tries to cover many topics and provide general advice, so it does not drill very deep into any single solution. While the advice is general, it stays grounded in practical decisions to evolve existing tools and avoids becoming an ivory-tower design lecture. Notable Callouts:
* Queue controls in front of every service help manage the service ecosystem, prevent traffic bursts from overloading servers
  * Instrument every server call, including the queues
* Wrote services in C## which made sharing tools easier - existing content toolchain was in C#
* Started with a single in-house datacenter for Destiny in 2014, cloud hosted more 'virtual clients' to scale for for Destiny 2
* Should have hardened separation between the (C++) game-logic that runs in a server and (C#) server-specific code
* Blob storate of data provided huge throughput gains over SQL indexing
* Migrate services with an A/B treatment, where you can dynamically scale up the traffic to the new service and run both in parallel. Can also quickly switch back to only the old service without a deployment.
  * Extra cost to temporarily run two services, but a huge savings in stability and customer trust
* Don't need to constantly update all server changes directly into database - we persist changes every 5 minutes, so that the player loses at most 5 minutes.
  * Understand your real requirements, and be skeptical of 100% targets
* Do stress tests, but don't be surprised if they don't catch everything. Queues can save you headaches both during testing and in live!
  * Test with real player data, not only with debug content
* Invest in making it as easy as possible for your engineers to do the right thing!
  * Example: make it easy to add performance counters, for user to record what they are seeing
* Showing named errors to end users was more reliable than error-codes, users would actually remember "error code weasel" accurately
* Try to understand your failure-space and have runbooks ready (and also create new ones when you find new failure types)

### Raw Notes

Action MMO, talk through lessons from the architecture, best practices

Brief History
Halo was the first stack that many experienced. Used XBL and also built out its own services. Stats service, fileshare (screenshots, saved films, edited maps)
Strongly favored Windows and C#, backed by MSSQL, hosted in datacenter ran by Bungie IT (less than a single engineer's time)
By Halo reach only 4 engineers

Destiny was always online, if server went down nobody was playing anything (Halo has a offline campaign)
Loot-based gameplay means larger player data to store
Multi-platform means high concurrency
Complex hybrid hosting plan, both P2P and dedicated servers
A single shard universe, with no artificial divisions

Pragmatic choices to simplify problem
Stick with windows servers, to simplify ramp up
Continue using C## for services, which the content tools were also written in, easy to share
MSSQL as backing storage

Destiny's services aren't highly latency sensitive, rewards and scripts are server-authoratative. Death animations are much slower than round-trip data
In 2014 would be challenging to run everything in the cloud! Can build out a single datacenter to control complexity.

Both Destiny and Destiny 2 had smooth launches. Though have had a variety of downtimes and rollbacks over the years.

Today Destiny is
40+ services, 18 SQL databases, etc

Was originally 25 microservices in first game, spending a single minute on each would burn all the time in the talk.

Talk through bootflow... one part is critical to success to the game: sign on queue and throttles. Not unique, but building this is one of the best investments, excellent tool for when something goes wrong.
You have a maximum capacity, even if you don't know what this is.
"Thundering Herds" of players can occur which are also a type of capacity limit, due to launch hype, or internet glitches... may want planned downtime
If your queueing system doesn't properly control the rate into the game, it can create its own thundering herd event
Queue responses have a unique message, which we rarely change due to localization impact - usually only during event downtime
Limit your queue and throttle dependencies. Need your queue to be working while everything else is on fire. Queue tokens allow us to recreate the state.
Put throttle this in front of everything, including auth
When you want to go offline, set queue to zero. Returing a queue response instead of an error message makes mistakes less likely.
Default population cap to 80% of known capacity. Be right about those limits before you hit them!
If an issue is happening: set pop cap to zero, wait for users to drain out of the system (kick if necessary), then slowly ratchet population back up
First and best tool to handle a live service

BAP (Bungie Access Protocol) Server
Stateful gateway to service layer, communication on TCP (if we wrote it again, would write in UDP)
Securely encrypted using the token provided in the sign on service

WorldServer
User data managing server
Character data worked on in-memory, one of few stateful services
Can handle ~5K accounts
Hosts C++ game logic DLLs that run on player data, same language between client and server
However C## is used around this for the server, and if we did this again, we would harden the separation between the game logic and the server.

Claims Service
Acts as a routing service to the stateful Worldserver, finds available room in a WorldServer for a user

Character PubSub service
Subcomponent of WorldServer, in-order differential updates, can subscribe to different levels of detail on any character
Given client can have many subscriptions at any time
Uses ZeroMQ, high perf brokerless messaging library

Character Storage
Between progression and loot, tons of data to store. At launch, was stored in SQL in a schematized database
Design would allow us to selectively update only data that changed, could run analytics queries across DB
Okay for launch, but a number of big problems
Heavy duty loads. Single character could ahve 6K account attributes, 4k per character attributes, etc.
Versioned content, as well!
Analytics queries were difficult. SQL fixups were terrifying, recommend avoiding this if at all possible.
Low savings from having incremental updates

Migrated to blob-store model. Though about having a mega-blob, but stuck with one per account, one per char, one blob for all items. Used SQL to store blobs, but could have use any k-v storage

90% savings in space, improved load times. Easily stores 200K+ rows per account today
New debugging functionality made possible, no major conversion dontime for users
How do you migrate this!? I'll walk through it.

As a first step, systems are always set up to write to both systems. We load from the old system, and then write to both (but have a flag to disable in case there are perf issues that impact players)
Set test acocunts to read from new. Look at edge cases such as rarely signed-in characters and many-hours-a-day players. Important that testers are extremely familiar with accounts, to notice issues. If it looks good, roll out slowly to the players.
A/B test in production by taking a sample of the users, for instance by hashing their IDs. Start ramping up slowly.
If something goes wrong, kick the sample users offline and have them reload with only the old system enabled.
Once we're sure the new system works, nuke the old data and enjoy the savings.

Data reliability
Character data is very important to persist
If a player disconnects we persist their data
But what if the worldserver crashes?
Constant write-through is expensive
Change logging is very complex
Solution: persist character data every 5 minutes, but also do an extra persist on important events such as getting an exotic item
Player loses at most 5 minutes of progress
Understand your real requirements, and be skeptical of 100% targets

Clans System can have many simultaneous writes, in an optimistic concurrency model
Can have a bunch of different players on a team, generating rewards all at the same time
Clan server can recieve action from WorldServer, load the data and version, run actions against the data to enable new flags, then attempt to persist ...on fail, retry up to N times else log
Used Redis for persist store, which has been frustrating
Need to decide when to version up old clan data, for players running on old content

Activity Hosts and Bubble Hosts, the game-script and physics hosts for the game
4 services, two hosts and two proxies. One proxy per machine, C## apps that route messages. AH&BH are cut-down versions of the c++ game client. Balances load to the servers.
Each uses different networking models. AH uses TCP, BH uses UDP.
Bonus host type: Group Activity Host, same code as activity host, but used for things like public activities - group gets a message for kills by strangers outside of the group

Topic Grab Bag
Stress Testing: on each annual release, have done a ton of stress testing. Allocate portion of datacenter as a stress cluster - partitioning tends to shake out almost as many issues as the stress test itself.
Cloud hosted virtual clients
Finds key ship-stopper issues
Drawbacks: Expensive, labor intensive, heavy maintenance burden as game evolves and changes, bots are an imperfect simulation of human players
Stress testing alternatives:
- running against a single service is possible, but doesn't catch contention between servers
- Rely on queues to protect you
- prefer soft-launching features and systems before fully enabled, like with the blobs

Cloud usage:
Destiny 1 was full mesh, peer to peer, NAT relay had to be geolocated
Destiny 2 allowed bubble hosts to scale into cloud, not geolocated!

User Error Reporting:
Instead of error codes, used names of animals and vegetables. Very good for search engines, and users remembering what they saw.

Service Settings:
Every constant in our service is handled as a setting, and guarded by a flag. Allows us to tweak or disable something without a code change.

Logging:
Send warnings with a lambda, only call string format if error level is active. Also calculates a hash of the format string for easy searching.

Invest in making it as easy as possible for your engineers to do the right thing! Easy to add performance counters, for user to record what they are seeing.

How can things go wrong?
When systems have been on fire, our service engineers had a tradition of putting on cowboy hats. You knew something crazy was happening.
Game-logic based data corruption, very dangerous but hard to catch. Example: player currency cap was drastically reduced, created longest ever downtime for a full world rollback. Mitigate: test with real player data, not debug created content, sourced from a variety of play styles. If you see corruption, offline players immediately, may be able to do a targeted rollback instead of full-world. Have a rollback runbook, can be error prone without guide. Investigate options of fast recovery for issues within a launch window
Data store performance degradation: growth can cause big perf shifts, new call patterns create new pressure, one slow query can slow others, finding real culprit nontrivial. Mitigate: use queue and throttle, disable optional systems during search, stress testing can catch these in advance, have instrumentation on every call, alert at dangerous thresholds
Retry-based death spiral: systems tend to retry on failure, can be automatic or player-driven, unexpected load due to more systems collapsing, queueing can timeout. Mitigate: simulate service failure during stress testing, if spiraling, offilne and ramp back up, where possible avoid retries and handle failure, prefer adding a backoff and jitter to retrying, prefer short queues with rapid rejection when full

Understand your failure space, and have runbooks ready

Future of services
Bungie Services not just Destiny Services
Need to support higher concurrencies and feature requirements
Further leverage the cloud, more servers in containers and on Linux
.NET core means we can get services over with minimal rewriting
Increase iteration and deployment speeds
Scale the team up


## Tools Summit: The Rust Programming Language for Game Tooling
Dan Olson, Principal Software Architect at Treyarch
https://www.gdcvault.com/play/1027315/Tools-Summit-The-Rust-Programming
### TL;DR:
Sales pitch for using Rust, but without concrete data! Supposedly near Python for code writing efficiency, and near C++ for runtime efficiency, plus improved security features.
### Summary
Due to developer interest, Treyarch engineers experimented with changing some of its tools starting in late 2017. Grew to 3 major tools and 20 smaller dependencies or CLIs. The results are not really investigated here! Error Handling and Multithreading are supposedly quite easy, but there weren't data or useful anecdotes that validate this. This talk was essentially a sales pitch on learning Rust, but without the sort of results-oriented data that could convince an organization or skeptical engineer to seriously invest in.

Supposedly after you make the investment to learn it, Rust is easier to use than C++ with better inbuilt dependency management, error-handling, and security features. Being a compiled language Rust is much faster than Python, however you'll still have one-time compilation and static validation checks that eat at least as much time as C++. While Python compares favorably to C++ for errors, Rust is even better for error prevention/detection and overall security. Like C++ Rust has more low-level "corners" than Python, though remains different enough from C++ that there remains a learning curve.

### Raw Notes
Core engine programmer, work on data pipeline and infrastructure tooling

Rust was started by Mozilla in 2006 with first stable release in 2015, focused on security and performance

The Case for Rust
Language comparison:
Python: easy to read, easy to write, errors sorta handled, performance is a mixed bag (dependencies fairly easy)
C++: moderately hard to read, hard to write especially to use dependencies, errors poorly handled and not standardized, performs well
Rust: easy to read, easy to write, errors sorta handled, performs well (dependencies fairly easy, though packages not as varied due to newness)

Sales Pitch
Rust is similar to Python for code writing efficiency, and similar to C++ for runtime efficiency
Large centralized ecosystem of communty libraries "crates"
Integrated build+package+test tool "cargo"
Static, compile-time validation of common memory and multithreading problems

Case Study: Image Packer
Heavily multithreaded tool for mastering images. Across three years have only hit two crash issues (one in a C library)

Survey of interesting uses: Rust for Game Tools
Error Handling: result (success/fail) and panic (unrecoverable error), questionmark operator passes failed result up callstack, can use with_context to add error messages
Multithreading: Crate called Rayon which makes multithreading easy, has a more functional style of declaration, Rust won't allow you to make mistakes like passing mutable data
Parsing Test: Crate named serde (serialize/deserialize) makes it easy
CLIs: Crate structopt to built options and help, automatically includes code comments
Parsing Debug info: Crates pdb and gimli (elf) to inspect debug info ...Activision has allowed us to open source this tool: https://github.com/Activision/structpack
C ABI compatibility: Bind rust code to other languages (python, nodejs, c, wasm)
Web Application: Lots of useful crates, rouille is my favorite, may want to use tide of actix-web for substantial apps ...but vet this strongly if it is public
GUIs: native rust crates (iced, druid, egui) C++ bindings exist but I recommend them less (ritual/Qt, relm/Gtk, imgui-rs, web-view)


Integrating Rust at Treyarch
Started work in late 2017, 3 major tools, around 20 smaller one-off tools, around 120K LOC, 27 individual contributors
Downsides: steep learning curve, very different from C++, complicated language with lots of corners (similar to C++), heavily reliant on ecosystem, compile times are as bad or worse than C++
Tips: communicate about what you are doing, keep it siloed off until critical mass of experience (start with updating unmaintained tool), start with best workflow (vscode + rust_analyzer + cargo-edit + rustfmt + clippy), bad Rust code still has same safety guarantees as good Rust code unlike C, initial hurdles are high but large productivity gains after, provide time and space to learn and teach (presentations, courses, examples, code reviews)

Dive into code: leetcode.com adventofcode.com
Dive into a book!
Dive into Online Resources!

...GDC seems very ASMR and low-energy, with headsets instead of podiums and lapel mics.


## AI Summit: Where The $@*&% Are Your Tests?!
Kevin Dill, Senior Solutions Engineer at Kythera AI
https://www.gdcvault.com/play/1027101/AI-Summit-Where-The-Are
### TL;DR:
Write automated tests! Start small, focus on what works best, circle back to pay debts you defer.
### Summary
This was a high-energy ramble across one engineer's journey to incorporating testing as a primary tool in their development toolbox. The advice is generally sound, though delivered as a broad scattershot of anecdotes. There is not much to sink one's teeth into, but it offers a tantalizing and healthy overview:

* Speaker had previously never found the benefit for unit testing in games
  * Clicked that there are benefits beyond tests verifying delivering a feature
  * (don't unit test the "game" portion of the game, unit test concrete tools and interfaces!)
* Near-instananeous test feedback helps you find bugs as close as possible to when they are introduced
  * Ideally when problem/context is fresh in your mind
* Tests efficiently help document the code, and reduce ambigity
  * Easier to "step right in" with a debugger to see how it works
  * Documents special cases (including newly found ones of the bug-flavor)
* Offers a safety net for when code changes, and code changes rapidly during game dev
* Critical to run every time fully automated, legible, and stable
  * Important to be fast, run in memory, and be atomic
* Modifying code to expose things "just for the tests" is a code smell! (Tests should use the public inteface as its consumer would)
* Can inject "parts of tests" into the code itself through asserts, errors, and warnings. Code itself can check its edge cases.
  * Tests can then focus on setting up use cases, and look for errors
* Don't worry about how big, hard, complex, or incomplete your code or tests are!
  * Start small: a line here, a bug there
  * When a test costs more to maintain than the benefit it provides, change it or change the code it targets
  * Sometimes you can't add a test now, but go back to add it in later
    * (Often becomes more expensive to add later, may require rewriting code)

### Raw Notes
Speaker Background: AI developer for 20+ years, "unit test curious" for 15 years, feel like I've never found the benefit for unit testing in games... around 6 or 7 years ago it finally "clicked" for me, as a team we are seeing benefits beyond writing the tests

This talk is about:
Why you should test
What I'm doing differently
Tips and Tricks (that work for me)
A few takeaways

Definitions:
GAIA: Game AI Architecture, I developed this at Lockheed Martin
CSN: City Scale Navigation, a new feature with graph-based spatial representation instead of navmesh, which I am working on

Unit Tests (according to conventional wisdom)
Automated code that checks a tiny unit of work, written alongside code in the development language, cover as much as possible, run every time you build and immediately tell about pass/fail, run in nightly to tell about errors in variants

Why Test?
Instant feedback (catch right away, best and safest time to fix a bug is just after you wrote it) like an easy button for bug finding (shorter repro loop)
Documents the code, step right in and see what's happening in the debugger, get back up to speed faster by looking at tests and understand how the code handles special cases
Safety net for when the code changes, and code changes a lot in game dev (this makes it both challenging to test AND powerful to test these areas)
If you don't have tests for a case, there's more ambiguity on how/whether something works

Point is to run every time, so it is critical to be fully automated, readable, maintainable, consistent, and order-agnostic! Also important to be fast, nice if they run in memory, and be atomic which I disagree with.

Exposing things directly for the tests is a code smell! Tests should use the public interfaces. How do we solve this? Inject your tests into the code: asserts, errors, and warnings should all make tests fail! Only allow success when they are expected. This means that the code itself will check its own edge cases. Tests then just call code, and set up edge cases that could trip an internal assert or warning.

Why we fail
Too big / too hard / don't know where to begin / angst about perfect coverage
Start with your next line of code, or the next bug you fix
If (Test Maintenance > Test Payoff) change your level of granularity, think about adding in errors and warnings into the code
Lack of buy-in/discipline... there are times when there isn't time to test, but you have to circle back



## Tools Summit: Embedded Scripting with Zero Overhead in Final Builds
Arturo Cepeda, Senior Engine & Game Programmer at Deck13 Interactive
https://www.gdcvault.com/play/1027069/Tools-Summit-Embedded-Scripting-with
https://github.com/arturocepeda/Cflat
### TL;DR:
Pitch for a domain specific scripting language based in C++, which was open-sourced. Your non-technical design team would hate it!
### Summary
Presenter wrote a scripting environment which compiles to use a hot-reloadable environment during production/iteration and compiles to specific instructions in release builds. This talk focused on how it was written, but not on the results of a team using it to produce a game. It seems like less-technical users of this domain-specific C++ dialect could quickly start using it "the wrong way" after following a tutorial online, digging into misleading IDE hints, or copying a chunk of code from another non-script-dialect cpp file.

### Raw Notes
Today's subject is an embedded scripting language with a parser and interpreter
Pros: Hot-reloading, Accessibility to designers and artists
Cons: Memory footprint (script env, loaded scripts), runtime-overhead (lookups, heap allocs, GC, execution of interpreted code)

Our current scripting uses Lua(v5.3) and sol2 wrapper for C++
Postmortem dissatisfaction: More people are used to C-style syntax, parser not helpful in cases such as incorrect arguments, memory footprint (critical on console), runtime overhead

Scripts do not need to be modified in final builds, scripting only has advantages as a development tool... All scripts are part of the c++ code? Write scripts directly in C++? Compiled my wishes of things we want from Lua, but still use C++ syntax.

C++ interpreter (Cling) has a ton of features, but is not lightweight and too many dependencies
Runtime compiled C++ (open source) but still not lightweight
Live++ is subscription for commercial projects
...Went with custom project, named it Cflat, open source (zlib), no external dependencies only C++11 headers

Bindings for calling methods with macros

Cflat comes with a header, an optional helper for files which access scripts

Creates script header in development build, creates cpp in final builds

Not threadsafe, but in global header can implement locks to environment ...which in final builds won't exist

Have execution of functions by name, for use case in custom nodes for visual scripting. For final build define fixed function signature, and then have a global dictionary of name hashes. Autogen a wrapper for each function exposed as a node.

Automatic hot-reload on script save, info-log on success, error-log on error and leave old version loaded. Use VSCode with a C++ extension, and settings saved in JSON file for scripting users to simply load.

Have a script debugger integrated into our engine

Avoid 'using namespace' statements in global scope!


## Tools Summit: LIVE Game Show

Zoom-hosted trivia game similar to pub trivia, very silly. I narrowly won it, woo! https://thetoolsmiths.org/2021/game-show/

## Independent Games Summit: You Have to Stop Working, Even When You Can't Stop Working (lightning talks)
(Various)
https://www.gdcvault.com/play/1027456/Independent-Games-Summit-You-Have
### TL;DR:
Don't work yourself to death.
### Summary
There's some good information in these talks, mental health, personally vulnerable stories, etc. but it is fairly squishy reflection and not directly actionable. You're going to need to do your own mental work to understand how to better balance your unique situation. However this type of talk is a good tool to prompt yourself for space to think through that work.

### Raw Notes
Corporate developers can get driven to burnout, and independent devs can drive themselves to burnout even more severely

Some places I worked at had mandatory crunch, but all had peer pressure crunch because the games have set deadlines. Regardless of constraints, need to make it as ready as possible.
Years of doing this take a toll, not healthy long term. After doing that for 20+ years, started a fully remote studio. Was learning how to run a company, and still doing too much.
A lot of my self-worth was tied into how productive I have been. I try to make myself more redundant, less critical for all decisions.
Working from home helps make time for activities outside of work.
Still a constant struggle to want to be pulled in, try to shut off work when I am not 'at work'
We have some 12-hour time differences between some partners, we enforce mandatory days off and try to make room for the stress of things like Covid
As a studio head, need to make a good example for the rest of the team
(stopped taking notes)

## Board Game Design Summit: Creating Immersion in Tabletop Games
Mike Mearls, Game Design Director at Wizards of the Coast
https://www.gdcvault.com/play/1027445/Board-Game-Design-Summit-Creating
### TL;DR:
D&D 5e was more successful than 4e due to following its roots while still streamlining the flow of the game. Shared immersion-keeping language is key to a successful tabletop game.
### Summary
I agree that the (remaining) unique component of tabletop RPGs is improv, and the ability of a system to bend to its players' shared imagination. Beyond setting up initial mental boundaries of engagement in a shared language, game rules can get in the way of collaborative creative discussion. However if there isn't sufficient linguistic agreement between a group of players (let alone a game's development team) there is going to be a lot of misunderstandings and frustration. Even since the earliest editions of D&D, some have viewed the rules as an impediment to player immersion. I disagree (as Mike seems to) that they are inherently opposed to immersion, but agree they can get in the way. One important thing that the rules can provide is a softening of interpersonal conflict, for instance: the dice roll killed my character because they were both unlucky and not skilled enough! Compare to: the storyteller, who I thought was my friend, killed my character just to tell the story they wanted!

Mike suggests that TTRPG games can be visualized as having three major layers: Players, Rules, and Game-World (characters and setting). While I think this over-simplifies some important and complex parts of why/how TTRPG games work, it is still a useful perspective to frame design decisions or critique. To improve immersion at the table, try to avoid using different unique terms for the same thing between different layers. The rest of the talk focuses on differences between D&D 4e and 5e, and sometimes uses this visualization as a lens. In short, 5e took a lot of steps to make the rules more concise and also more evocative of a fantasy world. Part of this involved re-adopting some of the old D&D Jargon that 4e had tried to spell out in simpler english. Not every change was successful, but the overall result appears to have a better reception.

### Raw Notes
Root of this talk is in the differences in 4e D&D and 5e D&D. Many people loved 4e, and many hated it, which felt strange. Many players kept playing 3rd edition. Many said that 5e felt extremely like D&D at launch, and the reason behind this is something I have put a lot of thought into.

I believe every designer has their own technique, and there is no one true way. For me, I believe that TTRPG games are fundamentally improv activities. Comes together when everyone interacts. Rules must pull their weight, nobody likes looking up rules (well... some people do, if not all the time!)

Immersion varies between players, designers. Take the time to define it, and how can rules design help? If the team isn't on the same page, can have a lot of un-fun chaos.

In the early days of gaming, immersion was seen to run contrary to the rules. Ed Greenwood wrote an early article about the rules being a distraction, instead of acting as characters in the world. The rules would be gibberish to those in the world.

A lot of roleplayers will say "I wish I could go back and play this game again for the first time, I would have fun again" whereas in games that are very skill-based, players get better as they learn more. In RPGs there's a sense that players can become burnt out.

I see TTRPG as living in three layers. Players, Rules, and Characters+Setting. Things like dice aren't directly part of these layers, just part of the rules.

Player
Why do they play? Person answers this.
How do they feel? May have a bad day, may be frustrated with the group.
What do they want?

Character/Setting
What can they do? If I'm playing D&D perhaps I'm very good with a specific weapon, or some magic.
Where are they, and what are the constraints?
What do they need in this world? Defines play and what's going to happen.

Rules
Defines limits, in Star Wars likely won't have napoleonic musket rules
Creates shared expectations between players and storyteller, which helps resolves conflict. A turn from so-and-so did this to, the die says my character failed to notice something. The dice killed my character, not the GM! (much better for conflict)

If layers are in harmony, players can think and experience emotions as their character would. Game state, via GM, can change in response to internal experiences on all levels. GM exists as an emotional modulator for the game, to have an improv environment.
This flow is a unique element of TTRPGs. Other games rarely allow for this in a communal environment.

Harmonizing the Rules and the World
Allow the player and character to look at the world through the same eyes, simplifies mentally sliding into one role or the other
Player understands rules, character understands the world (and vice versa)
Makes it easier to alter the play and break standing rules when necessary
Benefits: simpler, focus on what's important in the rules, allows for arbitrary limits (neat tricks of 'this works this way in this world')
Drawbacks: can raise complexity where world detail bleeds into mechanics (what if the rule is lame, who owns the rule in a design team), needs more buy-in (lots of unique elements is a barrier to immersion)

Examples from 4&5
Powers vs Vancian Magic: In 4e there are special abilities, some are conditional, some have cooldowns. Seen as too gamey by some players! Before and after 5e the magic was special abilities, conditionally useful, and has cooldowns... well it really works the same way! I think the power system was too smooth and made a lot of sense, didn't have rough edges that a magic system should have. Also was the same for different characters, so it felt like it came from the system or player, and not the character. The strangeness begs to be explained in the world's terms, instead of being more directed at explaining it to the players through rules. Same end, but tension between the layers.
Magic item progression vs Attunement: In 4th we assume that players will have gained a given power level of item by a given level. While this rule is aimed at GMs, the players didn't have an in-world reason for this rule. In 5e the attunement is a pure game design limit, which we tuned through playtesting. However we looped it though the world, where the character's soul can only attach to a few items. Made it faster for players to play, and also make interesting choices.
In 4e had well defined character roles, with intended... In 5e there is character-chaos, push players toward roles in text, but don't outline them as strongly. Roles are typically useful for the others in the party, but character creation is essentially a selfish act and this didn't quite mesh well.
In other games: MtG's color pie is excellent constraint and balancing of resources. Similar in Settlers of Catan, have a good idea of what play is going to be based on looking at the board.

Use world terms as game terms
Where possible, use them interchangably. This brings the layers together, even if we have to dig out the rulebook and focus on one layer at the expense of the other.
Benefits: makes learning a bit easier, smoother interaction at the table allowing players to speak as characters, calls out your complexity during development process
Drawbacks: Jargon hurts accessibility, not a perfect tool keep it resonant with existing terminology, can lead to confusing (if loved) terminology

4e fighter powers were missed out on in 5e: creating space in tactical situations. 4e powers had slightly confusing names, if they were simpler names (Shield Bash instead of Tide of Iron), it could have been more evocative instead of esoteric.
Shift vs Disengage: disengage can be to engage another enemy, in which case it is less descriptive.

In 4e we tried to get rid of some D&D-isms, but in 5e we embraced many of them again. Immersing in the game's culture is also important, with experienced audiences.

In Other Games
Alien RPG uses Stress, good name and descriptive of how the game's universe works. Gives a bonus to actions, but if you fail then you panic. Risk/reward tension.
Pandemic's infections and outbreaks are simple terms, and players can quickly pick that up and understand

Realism as Spackle
When in doubt, let the player strata fill in the gaps. If you don't care, let the players' understanding fill things in. TTRPG's have the GM as an outlet for this, understanding of what would "really" happen to help the player understand the same.
Would like to have a clearer set of expectations from the GM, D&D could do a better job of this
Benefits: makes learning easier, limits volume of rules, focus on what's important. Don't force all players to be experts in firearms, or as knowlegeable as the most knowledgeable in the group.
Drawbacks: your realism might not match mine, newer GMs can be overwhelmed from pushback or ambiguity, some players want rules for everything (or more rules), the enthusiast trap of one player projecting knowledge onto others

Jumping in both editions requires reading the rules every time, players typically prefer to roll a die!
4e Modifiers vs 5e Advantage/Disadvantage - a +10 to attack could be situationally good or bad. 4e often had half level bonuses, 5e had a bounded accuracy. Bounding the accuracy across the entire game helps enforce inherent meaning and risk, so it doesn't change over time as much. Also have a lot more factors to add in 4e, including doing more math. Simplifying to advantage, makes it a more spiky and obvious benefit, more similar to how we humans percieve our lives. Not necessarily intuitive that less attention to detail would feel more realistic.



## AI Summit: Automated Game Testing Using a Numeric Domain Independent AI Planner
Bram Ridder, Sr. AI Engineer at Rebellion
https://www.gdcvault.com/play/1027236/AI-Summit-Automated-Game-Testing
### TL;DR:
How (but not why) to Write an AI system to play your game as a player would.
### Summary
This talk covers practical examples of implementing a planner with the goal of simulating player's navigation and problem solving. This high-level planner coordinates lower-level actions through behavior trees. The talk's focus is on the AI architecture of the system, which was first written for a FPS game and then updated for an RTS game. There's multiple places where the AI needed extra design affordances, such as linking where on a navmesh the player can walk off down to a lower area. Significant work needs to go in to optimizing/simplifying the planning problem to keep it performant, and this system used Numeric Relaxed Planning

It's a bit difficult to synthesize testing takeaways beyond "implement this at the AI level not the input level" ...primarily as this is an AI-focused talk, not a test-talk. The benefit of using a general AI appears to be reusability of core tech between different games or scenarios. Seems like there's still significant effort to hook general AI to specific new actions and challenges, so it's unclear what the savings would be over hand-written test scripts. There's likely benefit in dividing up the problem space into layers of planner and behavior-tree, but a "black art" to finding right place to make a divide with no specific advice. It would likely be untenably high effort to maintain this system while the game is going through early design iteration. However this will obviously be more stable than any solution relying on record and playback of raw controller input.

### Raw Notes
While I am a gamer, I get more pleasure out of watching AI run. I got put in control of rewriting an input simulator.

Automate testing of games
Smoke test: load a level
Soak test: load level, test for memory leaks - more interesting data than just loading and quitting
UBSan test: check for undefined behavior (sanity tests)
Simulate input: Actually get more interesting data once you get into levels.

We create an e-mail using a suite of tools, including Kibana and Jenkins

Old input simulator:
Record input into a file, and then annotate what they were doing. Then start the game, and the script should do the same thing
Doesn't always work, levels change, recording may no longer be correct
Example where character starts at a different place, so they get caught on geometry during this test, then teleports to another location to run another test
Not indicitative of how a player will play this

Creating a new input simuilator
Minimal requirements to use for designers
Robust changes in level and game design should be allowed
Able to complete all possible games reliably
Solution: create plans using an AI planning system, execute actions with Behavior Trees

Key benefit of domain-independent AI planning is that it isn't built to be specific to a single game, also has application in real world: automated warehouses, surgery, traffic control, becoming more common in games

AI planning problem consists of two components:
Domain - a model of the game. What types of things exist? What properties/relations do they have? What actions can be performed?
Problem - the current game state. What instances of the types exist? What properties/relations are true? What is the initial state? What are the goals?
(Walks through a visual example of this for quite a while)

[what is the benefit of using a general AI here? reusability of core tech between scenarios? Seems like there's still effort to hook general AI to specific new actions and scenarios, where is the savings?]

If we end up in an unexpected game state, re-plan and re-evaluate the state.

FPS game. Need to manually link objective markers, augment existing objectives in the game, and link the navmesh for places such as drop-down ledges.

Evil Genius 2 is much harder to play, as you don't directly control the players (agents in the world do). Game is sped up, automation builds rooms and furniture. At some point we need enough gold and power, so the planner takes actions to do this.

How to model games?
Too low level leads to huge branching factor, in extreme cases planning individual key presses
Too high level leads to huge behavior trees, in extreme cases having a single action of "complete game"
Balancing this is a bit of a black art, need to add a lot of constraints to inform how the planning gets hooked together.

Once we describe the technology tree, the planner can come up with "interesting" solutions such as increasing the heat (threat) capacity in an area instead of directly reducing heat.

Optimising the AI Planner
Heuristics: simplify/abstrict the AI planning problem. There's a lot of them, but we used Numeric Relaxed Planning Graph Heuristic and Unachieved Goals Heuristic (ZA4). Take facts with values, add an action layer of possible actions, then create a new fact layer that includes the positive effects, and copy the previous facts over to the new layer even if they are not changed. Keep iterating back and forth between facts and actions until we find a goal-fact. This is an NP-hard problem, so we can't find an optimal solution. Calculate how expensive we think it is to achieve all conditions. Label certain actions as helpful, to speed up the later plannign process.
Need to prune the search space, to reduce the number of objects and prune actions that can never be executed, and actiosn that are never going to be helpful. Cosmetic furniture which doesn't impactfully change the functional game state.
One type of pruning is to create constant facts, such as what a research option in a tech tree does.
Only consider actions that can help achieve a goal, remove them from the planning space. In EG2 this got rid of 90% of the planning actions from the search space.

Since this experiment was successful, expanding scope to multiplayer testing and achievement hunting. Also pulling out better data.

Excluding crashes, found over 150 bugs in EG2. Mostly game design issues. Great buy in from team. 99% of planning problems take less than a second to solve, and take less than 20MB to generate. Sometimes takes 180 MB.

EG2 Game designers found that this tool helped identify death spirals and too-easy exploits.

Want to improve this by:
Making the planning more explainable, help tell the designer in human language why the AI failed.
Temporal planning, knowing about time could minimize actual play time instead of minimizing number of actions.



## Fair Play Summit: Tackling Cheating In Gaming
Andrew Hogan, CMO & Co-Founder at Intorqa
https://www.gdcvault.com/play/1027167/Fair-Play-Summit-Tackling-Cheating
### TL;DR:
Cheating happens in games
### Summary
This talk seemed to be a waste of time and I cut out early, as at 1/3 over it had barely introduced its subject. Perhaps the point is just awareness that cheating happens? One standout quote is "Cheating is no longer PC-only" ...which it literally never was! Introqa is a security company, so there's probably a thinly veiled sales pitch for their service I missed out on.

### Raw Notes
May suprise you that the way to stop cheating involves how we find cheats.

Among some people, there seems to be resignation about cheating as something that is just going to happen. Language around this also is too broad, from aimbots to data scraping.

Data shows that the #2 reason players would avoid a game is it having too many cheats, right behind having a toxic community.

...huge tangent about criminal activity and The Wire... "Cheating is no longer PC-only" (duh!)

Cracking the Frozen APK to make it play offline and without paying MTX attacks the business model of the game.

## AI Summit: Your Buddy, the Grandmaster: Repurposing Gameplaying AI for Inclusivity
Batu Aytemiz, PhD Student at University of California Santa Cruz
https://www.gdcvault.com/play/1027235/AI-Summit-Your-Buddy-the
### TL;DR:
Include optional game-playing AI to assist players outside your core audience
### Summary
Certain game control schemes are intimidating or impossible for segments of your prospective players. The ability to enjoy and progress through the game is severely impacted for this audience. Gameplay options and AI-automation of parts of gameplay can make the game more accessible. Note that there is a spectrum of gaming ability/disability, and this spectrum also varies over time: the player may only temporarily have lower hand-eye coordination, not have use of a hand while it heals, or have reduced color-vision due to modifying their play environment. Leave these options up to the player to enable/disable as they see fit. This was a pure advocacy talk, and how to design such an AI was left as an audience exercise.

### Raw Notes
Casual users:
"I would never be able to play PC games, there are too many keys"
"I didn't play games with my child because they made me feel incompetent"
Self: I found a game impenetrable or uninteresting, so I couldn't share an experience I was initially excited about

When a player is rejected from a game, they are also rejected from the communities and social structures built around them. Need to make this inclusive, not just for the games but for the people.

Inclusive design is a method to develop a diversity of ways that people engage with a game. I originally thought about this as a binary - someone has only one arm, or is blind. However disability arises between a person and the context they are in. Disabilities could be temporary or situational (or partial). Assist modes could be one way of making a game more inclusive.

Celeste is a 2D platformer that has an assist mode, which allows you to modify the game's rules to reduce its difficulty. Up to the player to make these choices. Helps reduce the mismatch.

Another assist mode I've used is in Mario Odyssey.  Broke my collarbone last year, and this assist mode helped me overcome that.

In one case I was playing a game on a projector, and during the day I couldn't see the colors well due to ambient light.

Deep reinforcement models have been able to beat humans (alphago, openAI dota2, deepmind SC2)

So if you have a competent game playing AI, which is essentially a black box. It currently can crush your player, but that isn't the goal we want to help. What could this look like? Perhaps micro-management of units, player focusing more on macro management. AI coach for a sports game, could indicate where they could have made another bettert decision. Outline optimal path. Help with decision fatigue when there are many options.

How do we come up with assitance methods? Design exercise: identify challenge types, construct design space, explore the space. Ask ourselves which mix of planning and execution (or other features) what this would look like. Perhaps you make Celeste a one-button game where the player only has to time jumps?

Does this work? I don't know, but I am working on it. [loads up unity and shows a greybox example]

If I wasn't a PhD student, I wouldn't be using reinforcement learning. This tech isn't there yet. I would use navigation meshes, or other simpler solutions.

However you don't even need hightly competent AI to do this. Or really need AI at all.

Conclusion: assist your players


## Game Narrative Summit: Turn Your Writers Into Programmers: Greyboxing Narrative with Story Languages
Jon Manning, 50% of Secret Lab
Ryan North, Writer at Dinosaur Comics
https://www.gdcvault.com/play/1027215/Game-Narrative-Summit-Turn-Your
https://github.com/YarnSpinnerTool/YarnSpinner
### TL;DR:
An open source narrative-specific scripting language to simplify your writing and design effort
### Summary
Yarnspinner offers a domain-specific narrative scripting language more legible and easy to organize than a spreadsheet or a general-purpose scripting language. Its lightweight syntax results in gameplay-ready dialogue that reads quite similar to a screenplay. Staying lightweight also makes it easier to iterate on narrative-design problems, including prototyping where narrative-gameplay hooks exist (greyboxing). The presentation is charmingly charismatic, which helps sell using a new language.

One presenter offered "Advice for tools devs: don't solve the interesting problems, punt them, solve the framework problems and simplify things for the users" ...which I mostly agree with, though such a tool could have gnarly edges and maintainence of inconsistencies if it consistently punts hooking up to any real-world problem. To be on your audience's radar, a new tool needs a clear through-line to solving specific problems and/or simplifying sets of problems. Doubly so if it relies on learning a new domain-specific language! Still, a humble prototype is a good starting place for iterating on any tool.

### Raw Notes
How to save time and money and write better stories with greyboxing.

Lots of dialog tools are node-based. Nodes on a graph, multiple options coming otu of it. Lots of professional tools use this, and thousands of internal tools. These are all graph approaches to dialogue. Difficult to write due to lots of steps to edit.

Classic workhorse: spreadsheets! Very good for simple things, but for branching they are less effective.

Is there a better way? Yes, sometimes!

Blend these approaches, have flow control AND ease of writing.  Yarnspinner, scripting language for dialogue. Sceneplay are good if it's like an interactive movie, moving between scenes. Not designed for gameplay. Yarnspinner augments this with a scripting language. Not the first to try this, lots of games have done similar things.

Keeping state info in your head in a flat document (spreadsheet or word) gets frustrating. Twine can be a bit of a mess of nodes. Can try to organize this along different axes to help with timeframe and different characters.

Yarnspinner uses twine-like syntax and screenplay-like semantics. A screenplay is valid code. Has occasional flow-control syntax.

Editor reads this and prepares nodes. At runtime executes the script.

Keeps branching simple, don't connect nodes, just use an arrow operator to prompt

Also use this for scripting cutscenes in Night in the Woods.

Light weight, so it lets you write the story with enough hooks for the developer to do more afterward.

Made a shell of a demo, and passed off to Ryan. Rapidly prototyped a multi-hour script with a three day structure.  Once I got used to the development cycle of re-launching to see the state, I asked for debug info to be added to the screen to show the state of some variables. Started debugging like a developer would, but still focused on writing.

Wasn't perfect, at one point I wanted to use an array of who has been talked to, but I could emulate that with a series of if-statements.

Realised that yarnspinner is a domain-specific language for story-specific games. Level designers are used to greyboxing the environment, and found that this is really a narrative form of greyboxing. Writer can set up systems they need, or conversely programmer can set up some functionality and add stubs for dialogue. Can temp-out different parts of the content.

Built a reputation system which controlled which ending organically, without having to rely on a developer.

Anything not user-facing is encoded in a hashtag

Don't need to nail down what commands need to be sent to the game, can be built as needed with 'command' syntax. Highly iterative narrative system.

Keeps developers from trying to 'solve' interesting problems that are not immediately useful.

This is less good at data that should exist in a table. Nonlinear data is hard to store in yarnspinner.

Advice for tools devs: don't solve the interesting problems, punt them, solve the framework problems and simplify things for the users.

(seems like it could have gnarly edges and maintainence of inconsistencies, since it punts hooking this up until later)




## Math in Game Development Summit: Even Faster Math Functions
Robin Green, Programmer at Pacific Light & Hologram
https://www.gdcvault.com/play/1027337/Math-in-Game-Development-Summit
### TL;DR:
Calling all FPGA experts: FPGA is on the rise!
### Summary
This talk demonstrates how to use FPGA to collapse some iterative mathematical calculations into single calculations, which is the bread-and-butter use of Field Programmable Gate Array chips. FPGA lets you program efficient hardware circuits for a specific calculation, instead of relying on the general purpose processor. Speed-hungry games will doubtlessly find a way to use them as they become more readily available (which is currently more relevant in the cloud computing sector). Another general optimization topic covered was storing precomputed tables.

This talk started deep in the math, and failed to guide audience perspective via motivation or foreshadowing. The content is better described as "FPGA tricks for FPGA experts" but it did not advertise itself this way. Unapproahcable, bewilderingly esoteric presentations like this give mathmeticians and programmers a bad reputation.

### Raw Notes

FPGA is a coming storm. Since we last talked in GDC 2002 numerical research has progressed. Talk aimed toward hobbyist and semi-pro projects using FPGA.

Squaring is a special case in multiplication. Visualize in bits. Squaring is essentially a stack of shifted additions.  Remove duplicated bits below diagonal line through area of bits. Focus on efficiency and cost, not latency.

If both operands are the same sign, can make operation smaller. For instance squaring a vector will always be positive.

If the range of input is small, can approximate with a polynomial.

In the 8bit days, multiply was exotic and often slow, so shifts and adds were widely used. Finding minimal number of adders needed is an NP-complete problem... but we can exhaustively search this space. Can produce recursive tables that produce an exhaustive expression, precomputed tables up to 12-bit integers.

Can merge multiple constant multiplications into a single trees.

By flipping MCM trees top-to-bottom, can take two inputs and make sum of constant products trees.

KCM constant multiplication can be broken into chunks and convert multiplication into FPGA LUT lookups. Useful for small multipliers like 3, 7, etc

KCM real multiply, can handle values such as multiply pi

Division by small integer, can create a table of quotient and remainder

Constant divison by using multiply-add, only produces correctly rounded quotient not remainder.

Integer square root has interesting properties, such as producing half the amount of bits entered.

Goal of FPGA is to use fewer resources.

Can sum or multiply bit heaps.  There's open source tools to help prepare this.

CORDIC (50 year old algorithm) is still the winner of FPGA use, algorithm stays the sae but you set up inputs and vaues differently. Uses preprocessed arctangent table. Occasionally need to fix up the values, since it computes a scaled version of the general solution. Often better done in another way on FPGA.

Bipartite numbers can store more accuracy in less storage, and allow parallel access. Introduces error, but bounds this error extremely low.  Tripartite tables start to eat into efficiency gained over normal multiplication. Can use general multipartite tables, which have interesting application (?)



## Math In Game Development Summit: Mathematical Analysis of Classic Card and Board Games
Nick Berry, President at DataGenetics
https://www.gdcvault.com/play/1027341/Math-In-Game-Development-Summit
### TL;DR:
How to model game outcomes through statistical modeling or via sampled data.
### Summary
This talk provided a comparison between using Monte Carlo (trials) and Bayesian (modeling) to model the outcomes of a game that depends on randomness. This was an easily accessible talk which required only a cursory math background. The games analyzed are Snakes/Chutes and Ladders, Candyland, and Darts; with other brief mentions about how to model Poker, Risk, and Yahtzee. While Darts is not specifically a random game, users with varying skill can be simulated by having a normally distributed variance around where they are aiming. For these games the statistics can outline some optimal strategies, as well as guide how a design choice can affect both the expected and edge-case player experience. However this talk focused on the how to model, collect, and plot data; and covers neither why to model nor schenarios where modeling data becomes less informative.

### Raw Notes
How do we decide whether a game is a fair game?  Two basic methods: experimentation / monte carlo OR formal modeling / bayesian.

Let's compare these strategies in common board games. How long does it take to play snakes/chutes and ladders? Shortest game takes seven rolls. To find the average, could use a random number generator and play the game a billion times. Modal number is 20 games!  Could also map cumulatively, chance of solving game in n-moves or less.  As a mathemetician, what kind of average are you looking for? Mode is 20, median is 29, mean is 36.2

Subjective approach: Markov Chains, model system as series of states and calculate probabiltiy of transitioning to another state, or staying put. Moving based on die has uniform probability, whereas deck of cards changes probability over time. Crucial to simple analysis that this is a memoryless system like a die roll.

Can then paint probability density based on starting state, and then start feeding ending states back into starting states. Keep doing this up to N rolls.

Candyland is not a memoryless system, has cards! Probability of drawing next card depends on what has been drawn. Cannot use Markov chain, but can use a "crippled markov chain" by ignoring cards already drawn. Won't be a perfect match to montecarlo simulation results.

Poker odds are quite complex! Many beneficial patterns, and odds depend on number of people at table. With two players, having any pair of cards is great. With ten players, having suited cards is more valuable.

Risk is a dice game. Better to be an attacker or a defender? Defender wins on tie, attacker can roll an extra die. Propagating this through, after 5x5 advantage is to attacker, before that is defender.

What is the probability of rolling a Yahtzee? Markov chain.

Where is the best place to aim at a dart board to get the best value? Depends on skill. Triple twenty is the best area, but elsewhere may be more advantageous. Model with high vs low standard deviation from average. Let's look at a heat map with a low deviation. High accuracy optimal starts aiming at triple 20, then lower moves to triple 19, and then eventually drifts toward the center - if you are very bad at darts, just try to hit the board!



## Machine Learning Summit: ML for Art and Game Balancing with Project Chimera
(Various from Deviation Games and Google)
### TL;DR:
Google prototyped a card game which heavily relies on deep-learning to interpolate through game content.
https://www.gdcvault.com/play/1027212/Machine-Learning-Summit-ML-for
### Summary
Project Chimera is a fascinating set of experiments in using machine learning to help not only with prototyping game AI and the results of changing game balance, but also with generating more visual content across the game. In both cases the machine-generated solvers / interpolators were heavily guided by expert feedback, and used as interative tools to tune and generate the content of a digital card game. ML-trained gameplay solvers is essentially the standard practice for applying ML in games: estimate win-rates and imbalances due to human-defined rules of the game, including changing the rules. While it is not "common" across all games it has multiple success stories. The game's design also purposefully includes the evolutionary mutation of combining creatures together, which made space for ML-generated game assets. There is a tiny bit of novel game experience this opens up, but I don't see anything revolutionary for gameplay or game system design.

The most interesting use of ML was in human-guided generation of art assets. These ML-interpolated art results need to be seen for proper effect and, while the featured results were likely hightly-curated, are fairly compelling. Most of the assets are in 2D, but they clearly experimented with 3D assets as well. There were many individuals helping curate and constrain the asset data, only a few of which were experts in art or ML. Google did not share the current cost of using the big data systems, nor the cost of the human effort necessary to tune them. If they are not hyping this aspect, I suspect the cost of this experiment remains exorbitantly high. While this likely compares poorly to current game creation techniques, it should become cheaper in the future.

### Raw Notes
Project chimera is a fantasy card game prototype. Take images of real creatures and mash them together. Designed card game with the purpose of showing off what ML can do. Exercise in extreme collaboration with ML.

Many things we did and planned to do, and while we could cover all of this it would take many sessions. Going to cover two parts today, generating creature card images, and auto-play and auto-balance.

Art goal: generate high quality images with less effort and time. Rough estimate that an artist may take 20-40 minutes per image. Artist+ML, 1-2 seconds per image of tweaking. Created a chimera painter tool as a proof of concept. Used adversrial GAN so we can make up stuff and try to detect made-up stuff from real / training set. Alter each side (generate, detect) based on which component is succeeding/failing more.

In some examples, legs of different sizes, too many ears.  Realized that model was missing structure, as well as types of poses.  Tried using 3D data, but again with not enough structure info - resulted in nightmare fuel.  Decided that we would create segmented data, to help network understand areas.

Creating a good dataset:
Garbage in ==> garbage out. Must have the characteristics that you want!
Variety is beneficial, background made for better generated data
Prune images carefully

Tuning the ML model
Altered "perceptual loss" which required altering ML model params, happens during training. Vastly changed the output!

Adding final touches
Addes several components, landscape made by separate ML model and retouched
ML generated creature
Card frame / text

We generated a lot of these! Some used artist-drawn segmentations, some were purely generative.

Background was generated based on paintings (the presented set are compelling, but probably curated)

Artist-in-the-loop ML workflow
Started with art direction, then build custom dataset from landscape most likely able to support the model - is this the right dataset?
Then prototype ML models for training and generators - does inference images meet artist's spec?
Finally polish, artist steps in and directly uses photoshop to reduce noise and artifacts

Building a custom dataset - started with an automatic query in our image database. Simple script looking through metadata tags, sizes, types, image license. Then from that set have artist hand-pick samples. Then we ask volunteers to manualy curate for a couple hours to select a narrower subset based on if there are things like buildings or people. Clean up after intial query.

Collecting more samples will likely lead to a better image. Some biomes like Tundra and Chaparral had fewer sample counts. Pruned tundra from final design due to this.

Some times models struggle with high-frequency detail like forests. Can fine-tune better for dataset by removing difficult images, standardizing training composition more.

Ended up chosing BigGAN-Deep, which can capture complex landscape features. Very large CNN means long training time.

Artists came in and modified them with color correction, tint/muting to help the creature art pop.

Tried to add in painterly style, but this didn't quite work. Still has potential.

Thought we could reuse bad results and paint-over images to fix them up, even with low skill shows a potential for reuse.

ML in Game Balance

Stress test typically involves  thousands of play testing sessions with users, incorporate feedback, repeat until designers and testers are satisfied. Our time constraint was 6 months to try to get ML to offer shortcuts to game balance.

Started before overall game had enough structure, so we worked on "game zero" as a simpler prototype.  Each card belongs to one of 3 factions with type of abilities. Each turn player declares a faction and can only attack or play from that faction. To offset first player advantage, player one cannot attack on their first turn.

Created a CLI version of the game which could be played by a human, a human-created AI, or the ML. Had four simple procedural AI's hand created. Training pipeline can use the procedural AI's match logs as training data. Later have model play itself, and/or older iterations. Did not re-mix in the procedurals AI's as this would cheat, and over-train on procedural moves.

Trained ML model to predict win probability. Designers could analyze aspects of the game, such as average game length, win % of faction, ability usage

Applying "game zero" learnings to "Chimera"
Game has more rules and complexity!
Same training pipeline as Game Zero, let's go into more detail on training data.
Convolutional neural network with image-like vector inputs worked better than a fully connected network in practice. Small model, able to run on CPU using Unity Barracuda. Trial and error to find a good cluster of features to group together 'spatially' in the 'image'

Can use model to show a win probability graph, or recommend moves

Balancing - use the trained model to provide insights
Game designers generated specific decks. Found that one deck had a 60% win rate against another deck intended to have the same power level. Certain cards were very good, such as cards being summoned into disadvantageous biomes. T-Rex creature was overpowered!  Added cooldown period between its actions.

Had a clear advantage to evolve chimera, but it wasn't occurring as often as we wanted. Reduced energy required to evolve.

Even after balanging one card, did not change win% between the two decks. Looked at another set of changes, this time targeting the general decks to make them more balanced. Overcorrected!

Wanted then to encourage longer games, proper summoning placement, and overall balance. Tweaked starting values, increased penalties, decreased gap slightly between creature power levels. Got win percentage between decks near 50-50 for the ML


## Machine Learning Summit: Smart Tech in 'Hearthstone'
Tian Ding, Lead Data Scientist at Blizzard Entertainment
https://www.gdcvault.com/play/1027403/Machine-Learning-Summit-Smart-Tech
### TL;DR:
How Blizzard added user-facing Machine Learning features in the digital card game Hearthstone
### Summary
This talk was a bit slow and repetitive, which could make it a decent introduction to ML. However, while it introduces some parts of Hearthstone, it would not accessible unless you are already familiar with Hearthstone or another deckbuilding game. It does not go deep into exactly how they implemented the ML. It instead outlines the ML structure and focuses on where it was applied and what its results were. The problems being solved are definitely appropriate for ML, but we don't get any decision making wisdom about where and why to use ML. Given the content, this should have been half the length.

### Raw Notes
Intro
Hearthstone has over 100M players, and has multiple different modes of play

Deckbuilding is essential to TCGs, but is not simple. Different modes have different methods/rules to build a deck.

Smart Tech at Blizzard
What is Smart Tech? Use data and rules to automatically achieve goals (during deck building phase)
e.g. build a deck around these two cards for a user, provide three options for a deck, adjust appearance of a card during drafting

#1 core value at Blizzard is "Gameplay First"

How can smart tech improve gameplay?
Help build a viable constructed deck: any player, any deck, any collection, any time (changing meta, expansions).
Provide interesting choice variety for dungeon-run deck building
Balance class winrates in drafting for arena

1. Smart deckbuilder
Builds from user's collection, constrained on both a user's available cards as well as on the user adding the seed for a deck (builder may just fill in a few cards)
Previous rule-based deck builder was significantly sub-optimal. Want to build a deck where it will be more competitive, give the user a better experience. This especially helps new and returning players.
Very positive reception to this feature since launch in March 2019, have updated algorithm constantly since then. Supports two pools of cards (standard, wild).
Response time is 3ms from the hearthstone client, to the hearthstone server, to the smart deck builder service, then back to the client.
Has two layers, one for the metagame, and another that backfills in generically good cards for when the user is missing cards from the current metagame.
Also have sub-layers in these layers, which vary based on player rank. Have different metagame and card utility based on player skill.
We analyze clusters of similar decks per class, this can help show metrics over time for deck types as meta changes.
Using clusters we use "other mathematical solutions" to make up Layer 1
Update ML pipeline on metagame daily, to have recent results
Layer 2 may need to revert some steps that Layer 1 took, and then will backfill other cards. Layer 1 is more focused on synergy. Layer 2 is based on card power.
Collect how cards impact a deck's win rate to inform card power.
Layer 2 also has mana-cost buckets, to collect cards based on their cost. (part of the metagame is to efficiently play cards, which maximises the tempo of your play). Want to avoid over-filling a single mana bucket.
Have hard rules and soft rules. For instance certain cards require the deck be made a certain way (no cards of X cost, all cards of Y type, no duplicate cards, etc). Softer rules are about scaling a card up or down from a game design perspective, some cards we want to show to the user more often because they feel good to play.
There's also a cooldown to recommending certain cards, so that every time you use the builder it will give a slightly different deck. Up to three times within 10 minutes.

2. Smart loot buckets in Duels
Call this bucket "Group Learning" which recommends a bucket that synergizes well with user's current deck. Learns synergy score from gameplay data - what is the win/loss after card pairs are played. Creates a pairwise synergy score. Normalize this for a fair comparison between pairs.
Good for both recommendation and generation.
Still want to adjust scores for mana curve and add a multi-appearance penalty
We also can weight the sampling, and control the randomness of which type of bucket appears. Keeps the gameplay fresh, not deterministic and stale.
Also took iterations of blocks of different cards from different expansions to make the game mode more interesting.

3. Smart card balance in Arena
Need win rate across classes in Arena to be as close to 50% as possible, to keep it fun
Achieve this by tuning how often each card appears. Small adjustments to certain cards can make big changes in winrate.
Use gameplay data, card info, and card pick-rate to evaulate cards.
First build a predictive model to predict a win for a given a card-draw, then use constrained optimization to find sets as close to 50% as possible. Use this to calcuate a weight on the cards.
Implementing this has a significant improvement on clustering win rates together. Previously had 37% to 54% variance between classes. When deploying moved to have 41% to 52% variance. Recent update had to re-tune our recommendations, with a 48% to 58% variance and one deck way in the lead. Now have a 44% to 52% variance.

Final notes
We designed our smart tech to be scalable to future changes
Try to reduce manual work or find solutions we can't solve manually
Evolve output as game evolves

## Automated Testing Roundtable Day 1: Process
Andrew Fray, Lead Programmer at Roll7
### TL;DR: Communication is key to finding the correct process. Test automation requires schedule space to deliver value.
### Summary
This process discussion was centered around making test writing accessible and producing stable data which is actionable. Different teams will have wildly different resources, hurdles, and outlook. Start small and (as stated in other roundtables) have process and tools meet the engineering problem "where it is" instead of taking an idealistic approach. Many are trying to leverage automated test writing across their team, some have a QA org taking this on, and a few rely on individuals that own tests.

### Raw Notes:

Andrew has been running this roundtable for about six years, this is the first virtal one.  Please bear with and provide feeback afterwards. Today is about process, talking about the people that write the tests and the workflows and processes that enforce the tests.  We don't stick strictly to this, other topics are allowed.  Don't need to have special qualifications to chime in.  Will collect topics initially and then walk through each. Link to website https://autotestingroundtable.com/ that tracks these topics.

#### How does QA department "sit" within different departments? How is QA included in processes?
We don't have the QA department integrated into other departments. Been trying to get QA on board with a feature that helps play the game, but have not been successful with engaging anyone to use the tool.  What does the breakdown between companies look like?
* Hard to add to someone's existing job responsibilites when they are already fully scheduled! If you are asking someone to write new test automation code in something simple, you can train someone up with sessions, e.g. show that different login methods can be covered with one piece of automation. Can help to demonstrate up-scaling to other skillsets in their job. Depends on what type of tool you want them to use, depends on what the bar is. Starting with nUnit unit tests and then working into integration tests could work.
If QA comes to you, and asks for a certain part of the system to be automatable, or to have ti be automated for them. How do you handle that?
* I think this relates to our struggles at Monolith. A lot of times your audience doesn't understand the scope of what automation means. Often someone doesn't know which parts of the system are easy to test with automation. Smaller, unit-level things like sign-in that are relatively lightweight are often better. Automation involves thinking about tests in a way that a lot of (manual) testers are not used to thinking about.
* We are trying to get manual testing more engaged with our automated testing. Trying to see if we can automate parts of the BVT that they do. BVT is our Build Verification Tests... though everyone uses this term differently. In our team these are the tests that make sure the build is even worth testing thoroughly.  Our game has a large surface area to test.  We have an initiative to try to get less technical people to test, taking some existing structure-block and javascript and seeing if this can be tested. Trying to set this up in-engine, though have not yet had success in this.
* On FMOD we have one person able to write tests on our team, have a JS scripting framework embedded in our tools. As far as getting tests in to perforce, we work with this person to review their code. The tests come from them, they can direct where their testing comes from, to work on broader scale tests.
* Whenever I hear conversation about what QA should test and what Automation should test - you have to get up away from your test to talk. This is a discussion about what humans should be testing, and what things automation should handle. A lot of times, this comes down to how can we save QA time. Make sure QA leads and directors are aware where the QA time is being spent. Optimizing this, speeding them up to the human-only tasks is important. Like any other tools process, do the simple things that aren't necessarily test-specific. How do you move more gigabytes faster to a machine, so people wait less?
##### Asker's biggest takeaway: create a framework where QA can write their own tests.

#### Performance testing and metrics, instrumented markers. Interested in gathering and comparative analysis. What issues do you have to get reliable perf metrics?
Trying to move the needle in Unreal about gathering profiling data, trying to run this isolated from incremental states and caches. Trying to make performance A/B comparable, and to plot changes in this data over time. Turning into a challenge in getting signal-to-noise to be useful. Any great tricks to get this to match up?  Could be things like burst clock or cooling factors that could impact our data.
* Since my team (FMOD) just works on audio, we aren't maxing out perf when we run tests. We will take a replay that we have gathered somewhere, play and record with markers.  Turn this into data and visual graphs, we don't get exact numbers but this is good enough to show a section of the code over a period of time... we use that to make decisions. Run to run this is significant enough from a large capture. Frame to frame we have less inspection.
* General approach we have is to run automation on machines, and then we ingest the data from that into splunk. Did a lot of work to make it consistent, but did not get there. For example we cannot control how hot the devices are that are stored in our device-provider's lab. Variance made it hard to know whether it was a directly-caused event, now trying a moving average alert to warn about these. Also have a last-known good for perf stats, which sets a threshold for a hard-fail on performance.
* From our thousands of events, we whitelist to the ones we think are important, and then we save those as blessed values. Trying to get the data from each bulid to be within thresholds. 
* If you are trying to get something as stable as possible, instead of going for highest perf as possible. Try to disable all extra features on the machine, target your lowest spec.
* May try to disable more system features
##### Asker's biggest takeaway: Like the idea of running something pretty often. Trying to make sure it doesn't get lost in the high iteration rate. Like the idea of limiting scale and increasing frequency. Perhaps signal evens out with greater amount of data.

#### Pre-submission tests
In big studios with lots of changelists, we go through a helper that checks things before submission. Important relevant part is to have local or jenkins-based test run before submission, some places have humans go manually and check before submission. I currently have to wait half an hour, where I am effectively blocked.
* Don't have all tests run in that pipeline, have some builds and tests run in nightly, or in a slower periodic cadence.
* How regimented do you want to get? Spending a couple years trying pre-submit testing, and then showing on a timeline posthumously who ran tests before submission. (team had the ability to opt out) This way QA or other orgs know where to apply social pressure for where to change behaviors. A lot of times teams have a good idea of where to run more tests, allow them to opt into running additional tests beyond the 'simple' tests. Latency would be higher, but can use this as a feedback loop and provide more than just one presubmit-for-everyone. Provide a quantifiable "test this more" suite.
* For every change you are checking in, you should run the minimum set of tests that prove that the build is valid. Send a clear communication that, even if you don't have an automated way for having this run, provide a checklist of what to do during submission. Developers do want to make sure that they don't block everyone, that's a stressful situation. Having some sort of proof that you have done validation before submitting, you have proof to provide to a client before submitting to a code review. Coverage analysis as a prerequisite, not a code coverage number, but provide feedback to a developer before submission to understand which validations they need to add.
##### Asker's biggest takeaway: trusting the muscle memory of the user. My instinct was to try to figure out what has changed. Breaking apart the tests is something that I have seen, but that's a challenge to know how to break them apart will do this on a new project starting up.

#### Strategies to get dev team invested in the automated tests.
At Monolith we have our own engine, which is not out-of-the-box plug and play for testing. For those who have started the automation initiative at their studios. How have you gotten development teams to the point where they don't need QA to show up at their desk to know they need to fix a test.
* Anecdote: I read a CS education paper, where author divided class up. Half of them in IDE could run a single button to run tests, another had the tests run automatically on save. They closed this down as it was unethical to continue the experiment with how productive the auto-on-save was.
  * In regard to that CS experiment anecdote: keep this suite segmented to only fast tests. If you lock someone's machine for half an hour on save, they will have a very negative reaction to the feature!
* Basically am talking about this topic on Thursday! (Mojang talk) Need tools that are friction-free, especially when learning a new skill. Quite a lot that goes into making this successful.
* On a previous project automated testing started slowly, identified early the dev-testers who would be motivated. As a tester, work tends to be repetitive. Analyze from them what they would like to spend doing less, and then in other time trained them to do coding. Many of them didn't want to learn coding, listened to them and they wanted to do an atomic-level of development. Paired these individuals with testing. After seeing the benefits this was more 
* One thing that worked for my dev team was showing the Sea of Thieves talk, showing their chart of bugs over time.
##### Asker's biggest takeaway: go to Henry's talk on Thursday. Need to get people invested on their own terms.

#### How to get automation implemented early enough on to get savings down the road, due to tests not being a short-term development.
I now have a small team, how can I push automated testing as a way to move forward? Any open source tools that can provide support? Using Unity.
* Leaning on Unity's testing tools to get a smoke test running straight away. Unity can autogenerate a simple test that loads a level, checks for errors, closes the level, and checks results. After getting your foot in the door, adding a little testing in for different areas becomes easier.
* Do you want to ship higher-quality bug free software, and what is that worth to the company? Do you want to improve the work-life quality of the team? What is the value gained from improving the testing infrastructure? These are all noble goals, help make the case that these are important costs and benefits to approach and where. Would be great if someone could share a redacted pitch they have made.
* I was foundational testing engineer at Niantic. Anything not part of the automated process... looking for easy developer adoption, developers already writing in C## and using unity. What they need to do is to make the same calls in the CI/CD pipeline. Build an internal cloud farm for that, available on every check-in. Need a placeholder to deploy new code. Even if you only run nightly, make that part of a pipeline and get it running. Team will start to see the value by itself. Use existing libraries to run an integrationt est. One issue between server and client side, is that you can get blocked after shipping a change. Be able to provide a pre-deploy validation.
##### Asker's biggest takeaway: start small but smart, early.

#### What sort of tools are available for automated testing on target devices?
What tools are being used for on-device testing? You do not need to automate everything to run on-device. 80-90% of tests can be run in a certain way. Though running on another OS can then interfere with test accuracy.
* Using AWS DeviceFarm to be able to rent and scale up devices being used. To save on costs, we grab a few, high and low end devices. Then we run this every few weeks. We have engineers who do this manually locally, but we also want to get more data.
* We have a custom code base, so it has a custom framework. Framework is built into the game itself, can just launch the game and then start the tests internally.
* How do you make sure test framework is not a vulnerability running inside the game (for minecraft)?
  * Get the test framework compiled-out of the build for release
##### Asker's biggest takeaway: expose hooks for game to be used. We use nUnit as the harness and then a device farm in house.

#### Automated reporting that is correctly noisy. Too many notifications start to get ignored.
There's a fine line between sending timely notifications and sending a wave of spam. How do others make sure notifications are useful?
* We send one email per day, that only contains failures. These tests failed on this platform, so it doesn't take too long to grok.
* When you are dealing with a large volume of data, have to pivot more toward summarizing data. Change representation fo dataset.
##### Asker's biggest takeaway: stay in communication with those recieving notifications to find how to tune the summary.



## Technical Issues in Tools Development Roundtable Day 1: Assets
Geoff Evans, Tools Engineer at Epic Games
### TL;DR:
Devs gripe about asset management and script simplification. There are no easy answers.
### Summary
Quite a few questions were asked here but not necessarily answered, such as how to have non-technical users rely on Git for managing assets. There's similarly some anxiety about tools with source control integration and how aggressive to make file-watchers; however the audience is primarily engineers who are comfortable with using the CLI.

Most devs currently write tools in Python, C#, moderately fewer in C++, and rarely in other languages.

Universal Scene Descriptor (USD) is gaining popularity, including to go between Digital Content Creation (DCC) tools. The schema subset of USD may be useful for accounting for game-like portions of asset behavior such as scripts that govern hair physics collision. Ubisoft open-sourced a windows explorer shell extension for USD: https://github.com/Activision/USDShellExtension/blob/main/docs/FEATURES.md

Combining meshes together is great for perf, but if you you don't track what original assets were coalesced will become a content update/iteration nightmare.

Common open source tools include VCPackage, AssImp, PyLance. Python is less "duct tape" when you add in type hints and linting, and treat it like a real language instead of just glue.

Simplify scripting problems with interfaces to lower-level code. Don't leave all problems to be implemented in a slower scripting language, or worse a gnarly nest of visual scripting dots-and-boxes. Transition parts of scripted solutions into code, into tools that scripts can call.

### Raw Notes
Previous talks available at https://thetoolsmiths.org/codex/gdc/roundtable/technical_issues_in_tools/sessions

First zoom-based all digital version. Apologise if this isn't as productive as our frenetic in-person session.

This focuses on managing assets, controlling their version, etc.

Topics:

#### Survey of what is used in version control. P4? Git with LFS?
Around 1/3 use perforce, around 1/5 use Git and all use LFS
Platinum games uses pathos, and starting to transition into using plastic SCM
New version control tool called Snow

Programming languages:
C: around 1/3
C#: around 1/2
Rust: around 1/10
Zig: nobody
Python: around 1/2 "("the duct tape of game programming!"


#### On version control, anything in terms of integration into your tools
With FMOD we have integration with P4 and SVN, have lots of people using it, wondering how we can improve usability.  Interested in any wins for tool integration into source control.

Our integration is so deep that P4 attributes are used as our asset metadata storage. We do all the queries locally with a SQL server, otherwise it (connecting to p4) would be too slow. Results in one filesystem watcher for all art.  Every machine batch-fetches from P4, 90% of tools talk to local SQL DB and don't need to contact the server.

Git: how do you make it useable to non-technical users? Most people use our application and don't directly see Git. Our workflow isn't perfect, often have support requests such as VPN closing during sync.

#### How assets are stored and maintained
Blender plugin to swap between tools (it and unreal) quickly

USD (Universal Scene Descriptor) gaining popularity, also from DCC to DCC. Autodesk is putting a lot of effort into plugins for Maya and 3DS. Houdini does this well. In unreal going to unreal works, but going back does not work properly. Working well in the VFX pipeline.

We use FBX format in our back end.

We have a problem with USD is that we end up in situations where they want to recreate a scene, or view a character in context. A lot of create character assembly at runtime, but an artist wants to fix something wrong in Maya with hair colliding. Even though assets are scripts, not sure if we could build a USD structure and load it up even though it isn't checked in as USD. Ripe territory for the schema subset of USD.

Anybody worried about using USD, for instance houdini integration, if it doesn't pass through perf... controlling how when people produce content, but may result in something that will be nonperformant. Things that work well in the DCC tool but don't work well in the game.

We created a world validation manager across DCCs that can be used for any file format. Could be FBX, USD, extend to validate a world scene.

Activision USD Shell Extension https://github.com/Activision/USDShellExtension/blob/main/docs/FEATURES.md

#### Trasitive dependency tracking, sometimes something will change many layers away and how do you know? Dependencies that depend on other dependencies, and whether data is stale.

Our game has the ability to combine static meshes into a single mesh, but starts to loose its historic data. Data gets out of sync. Try to annotate where and how assets are being made, so you can go back to the changelist and author.


#### Open source libraries or framework?

Microsoft VCPackage for C/C++ package manager this has worked well for our open source libraries, worked at Unreal

O3DE is using assimp, the Open Asset Importer. More than a few also use it.

PyLance and static typing, python ceases to be duct tape when you use type hints and linting. Helps avoid the nightmare cases of stale code, e.g. old versions of python.

#### How many people like or prefer using integrated version control?

We do a half way. All of the add-to and edits for perforce are automated in internal libraries. Actual submit is either done from P4V or through a submission tool for those who don't want to use P4V. Do you have a trick to know if they do something outside of your tools? We have an aggressive file watcher to help find when there are changes pending, and to close open documents and reload them. Currently closes without saving / clobbering.

I use CLI because that's what I'm comfortable with, but this is hard to ramp up on for those who don't commonly work from the commandline. This can lead to degradation from not dogfooding alongside workflow changes.


#### Scripting, programmers write code in C++, looking to turn those scripts into an asset. What does your scripting system for engine look like?

There was an earlier presentation on Cflat talk, directly uses C++ under the hood with hot-reloading. Means your design team would be using a dialect of C++

We use a visual scripting tool similar to blueprints. During development they are XML files that can be parsed and diffed. Eventually get plugged into built C++ files and compiles them for launch.

There are counterintuitive ways such as shipping an interpreter, and then profile the code to find slow calls. Can ship game new data over a socket if you have this. Of course cannot do this on iOS. (I consider this a security risk!) A few constrained languages like Lua are allowed on iOS.

Most people would have visual scripting on one end, and c++ on the other end. (make interfaces that scripting language can call)

#### How to transition between visual scripting and transition this back to c++ code, or vice versa

We often have an intermediate representation, and visualizing raw C++ in a high level representation. Can attach a debugger and make sure it is doing the same as textual representation. If you have bespoke scripting system, really difficult to transition. Off the shelf tools may be easier to use for transitioning.

Find parts to isolate across an interface. Things like algorithms or loops, for instance pathfinding, are easier in code and difficult to write and maintain in visual scripting. Plus will be non performant in script. Transition pieces prototyped in scripting language to be called across an interface, which will simplify the scripts and make the problem more performant.


## Tools Design Roundtable Day 1: Design
David Lightbown, User Experience Director at Ubisoft
### TL;DR:
Design exercises are important, and good communication solves most problems.
### Summary
Around half the participants directly work on tools, joined mostly by managers and art/ux. 

Not many tools are localized into multiple languages. Slow-changing UX workflows can mitigate language barriers, as can use of symbols or colors. Team culture can also mitigate this - have a buddy to help guide you, or to swap blocked tasks with.

Hard to make sure that the design and UX of tools doesn't starve, especially on smaller teams or teams in flux. Stickynote brainstorming sessions are important for design, but difficult when remote. Ubisoft uses miro, which is a whiteboard-like tool.

Switching hotkeys on your users can create a negative surprise! Allow for saving hotkey profiles if you can. Announce changes/deprecation often.

Common UI/UX prototyping tools include Balsamiq, Adobe XD, Sketch, Figma, draw.io, Zeplin.

### Raw Notes

https://thetoolsmiths.org/

Around 50% tools programmers
Around 10% focus on the UI of tools
Around 15% tools team managers
Around 5% technical artists
Around 5% producers

Focusing on design, which is more about where in the UI it should be. Tomorrow is about production and structure of the tools.

#### How many need to support more than one language in scripting: studio uses both japanese and english. We know we want to support both, but do not have the time to do everything.

A lot of the tools we have worked on had to be translated since they were used internationally. Used our localization team, gave them a preview of how the game will be localized, used same system of string tokenization for them. Used Balsamic for the UI language, which is about how you flow the user through the tool. Code certain buttons in colors, not just language. Make sure that it is also colorblind accessible.

Unreal has a localization tool you can use, look at the unreal editor source code.

People tend to remember where things are in a UI based on position, moving buttons around is painful. If you leave it in the same place, can change the label without disrupting them.

Could rely on team culture instead of tool, have a buddy system of who to reach out to with questions, or to pass off certain tasks when blocked.

#### Keeping tools lean, moving to a smaller studio where we have to pick our battles. Need to pick features that won't die when abandoned. Knowing when to say no and which ones make sense.

Keep it modular so it stays clean. Think about all the parameters for future use, so it is easy to build upon. Keep it as a separate note, but don't necessarily link it together.

#### How to instill design culture when there is team flux

Dedicated tools design UX research is very beneficial. Dedicated person to talk to people as full-time job, looking through other tools, a person with this design vision and knowing the principles will get you a lot of culture. Will push in the right direction.

We don't have a dedicated UX designer, but have a tools/ux team that talks about UX.

Hard to have people juggle multiple roles. Whether you are time slicing a small team or have dedicated people, what makes it UX is that it is driven by the needs of people. Need to make sure it doesn't starve. Need to also make sure you make investments into the underlying tech as well and not just getting the feedback.

#### When people start thinking about UX

Stickynote session, get all the stakeholders together to clarify how users are introduced to the tool

Hadn't been in a real stickynote session until I had lurked on one. Would love to know resources for UX stickynote sessions. There are books about this, was a session from two years back: https://www.gdcvault.com/play/1026441/Tools-Tutorial-Day-The-System
https://thetoolsmiths.org/assets/gdc/2018/LessTimeAtYourDesk.pdf
https://www.gdcvault.com/play/1025287/Tools-Tutorial-Day-UX-Microtalks

Need to go through cycles of brainstorming more ideas, and then narrow them down to vetted ideas.

Hard to do from home! How can you do that from a hybrid or remote work environment.

Use Miro at ubisoft, a whiteboard like tool

#### Strategies in switching hotkeys in editors

Was developing an automated tool, and one of the software changed the hotkeys and the automation ended up deleting all of the assets.

Deprecation, have hotkey profiles saved. Allow multiple hotkey combos for same functionality.


#### Prototyping in general for a workflow

Tools: Balsamiq, Adobe XD, Sketch, Figma, draw.io, Zeplin

If you think about user-centered design, there's no good or bad time to begin, keep repeating and iterating

Adobe XD has the lowest barrier of entry - free to use!

Interested in a combination between Sketch and Zeplin, though failed to get team to adopt this


## Scriptable Core Dump Debugging and Memory Stomp Tracking
Artem Kovalovs, Graphics & Game Programmer at Naughty Dog LLC
### TL;DR:
Python-based scriptable debugging and data-querying in a Visual Studio extension.
### Summary
A currently-private Python tool was demonstrated, which allows the user to script debugger commands and memory address queries for the C++ application. This complements the MSVS watch window, by allowing the user to more easily search through all the data in a live app or core dump.

By quering for forensic data, Naughty Dog used this to narrow the candidates of where a memory signature was stomped from. Allows users to mine crash dumps for data they are interested in. Can also set up queries to collect low-level data while a system is running, to accumulate more evidence that led to a crash. Didn't create specific tools for these use cases, as bugs are generally too varied to ccreate a specific tool for each.

Did not mention if an open source version of the tool will be released. Their tool is likely full of closed-source console integrations.

### Raw Notes
Tool for forensic investigation of crashes

Programming is fun and exciting, as developers we often spend more time debugging than programming.

Motivation
Many tools to track down causes of bugs, most bugs are straightforward but some are very hard
Scope of problem can be very large

IDE watch window, Natvis to apply rules to view data

Problem: More complex debugging
Complicated problems can't be solved in a watch window
e.g. Thousands of blocks in a buffer, do any share the same unique index? Are any materials referencing a deleted shader in thousands of materials?
These kinds of issues require cross referencing of data an mining information from a dump, cannot be done done manually

Can try to guess what the issue is, can add runtime code to trigger assert, log/print more
...doesn't work for everything, sometimes crashes are different every time, or checks necessary in many places
Sometimes adding error checking logic changes timing of execution, sometimes bug is too rare to rely on repro
Due to this, we need post-mortem debugging

VSIX C## plugin for accessible functionality, look at threads and callstack, command evaluation similar to watch window. Have break/resume execution. Though bugs are too varied and complex to preemptively create a plugin for. Can't have a button to solve everything.

Instead really need a scriptable interface.  Can have a python client that can connect to C## plugin and execute commands, this way plugin is stable but scripts can change on the fly.

Can use the VS Python IDE, hook into python inside MSVS. Have commandline as well as see scripts running.

Python C-like API
Assemble a string to evaluate in python and send to plugin, we could stick with printf evaluations, but can make it nicer
Can write code in Python that looks like your game code
Can automatically assemble a string on the plugin side to evaluate based on the Python code

Benefits:
Looks more like real code, easier to copy/paste from C++
Autocomplete functionality is possible, from python object inspection being hooked up to querying the plugin
Query struct from plug-in

Quick iterative script development demo
Can test things out interactively in the Python terminal, to help us build out our script
Can copy-paste results into watch window after we find what we are interested in
Focus is on interaction between watch window and python interactive console, faster than just digging through watch window
This is done without modifying the code, all using just the crash dump info loaded into MSVS

Use Cases from the real world
Particle data analysis, had a crash in the particle system after running for hours
Theory is that a unique ID counter has wrapped to an old block
Iterated through list of blocks and printed IDs to confirm, blocks were binary packed and non-uniform, very hard to view in watch window

Profile data / job execution
Engines usually have profile data we can look at, but we are on breakpoint or in crash dump, so we can't view that data.
We can write a script that will grab the data from the executable/dump and show it in a different window. Based on variables, print them into Qt to visually show the data.
Every frame is different, how things are scheduled can vary.

Large data parsing optimization
Evaluation of one value at a time is slow if we are going through many evals
We use NdiDebugger to dump memory chunks to files and load back into Python (for speed)

Memory Stomps
Some location in memory is overwritten, can result in a crash, infinite loop, error
Notoriously hard to track down, eacn stomp is different
Give developer the tools to track it down!

Investigate and determine memory has been wiped with zeroes... likely that whoever stomped this has this location stored in their variables! Let's search through all memory for location X, or close to X.
We use NdiDebugger plugin to grab all memory. We might get many results! Closer they are to the pointer, the more suspicious they are.
Polymorphic object could store X at location Y, we could look at memory at Y-4, Y-8, Y-12... to find vtable pointer. If you cast a value to a void* then debugger will recognize it! Then inspect object to double check if it has a pointer to what got stomped.

What if we don't have a vtable? Perhaps try to compute every 64-bit object into a string, to check if it is a setting? Search through everything suspicious reference in hours instead of days.

What if we can't find location X? Can look for memory pattern that got stomped. Is there anywhere in memory we store the exact same pattern that was stomped with?

Was very useful for us to mine crash dumps to find important data. We made this C++ like so it was fast for existing programmers to onboard to. Opens the door to different debugging techniques, postmortem debugging

From chat: "basically wrapping the debugger functionality in a RPC server that's a VSIX"





## An Open Source Foundation for the Game Industry Roundtable
Stephen Jacobs, Professor of Interactive Media & Co-Founder of IGDA Open Source SIG
Ruth Suehle, Director of OSPO Community Outreach at Red Hat & Co-Chair of IGDA Open Source SIG
Chris Aniszczyk, Developer at Linux Foundation
### TL;DR:
Open source is growing in importance and steering committies/foundations make them work at scale.
### Summary
This was not facilitated as a professional roundtable. The organizers spent the first third of it directly calling on the faces they knew, which set the tone for discussion being more exclusive and panel-like. Result was lots of name dropping and plugging of projects, and not much shop-talk around current challenges in the field. Not great optics for an outsider!

Open source helps tools live beyond the scale of a single game, engine, or organization. Important for academics as managed computer labs with licensed software is not currently feasible. Reducing costs is obviously attractive. Students often want to use the proprietary tools as those are what are commonly used in the industry. Want to highlight that open source tools are also used in the industry! Challenge to help new engineers and artists understand how to take knowledge of one tool into learning another tool. (IMO this comes from student-perception that undergraduate education is best for learning specific "endgame"
skills, instead of for learning how to learn new skills)

Meta-gaming features (achievements, cross-game experiences, streaming, communities) require collaborating between companies, and open source standards and tools can simplify that.

It's hard to get companies to understand how to properly use open source software so "we have to trust they will figure it out through painful missteps" ...seems like that strategy alone could backfire! Regardless of company involvement, many individual engineers do not understand open source. A good way to get them started is to simply have them contribute to something small that exists--may activate their hacker instincts. Open source foundations are a good place for business and legal questions. In many cases, being able to point to a success is the best motivator.

### Raw Notes
A couple years ago, had been interested in video games for a long time. I used to do old-school professional game competitions, Counterstrike.  Got contacted by Royal O'Brien from AWS about Lumberyard, trying to get an open source project around it. Aligns with my interests at Linux Foundation about getting more gaming into open source, we have another initative in film.  Contacted Red Hat about open source, trying to know if they do open source related to games.  Blows my mind that developers are making games in windows with crazy tool chains, which I thought went away.

Jim Jeffers mentions Academy Software Foundation (film) and O3DF, talks about partnerships. Looking to create an open asset repository of digital 3D content for data in games and film. ... Open source can win awards, academy award. We believe they embraced the tool because it is open source.

I haven't had many open source experiences at GDC. "Oh linux, my dad uses that" If you're not able to game on it, you would be less likely to develop on it. Steam console is good news for Linux.

(This wasn't initially facilitated as a professional roundtable. The organizers spent the first 30 minutes calling on the faces they knew, set the tone for discussion being more exclusive and panel-like. Lots of name dropping and plugging, and not as much discussion of shop-talk.)

Historically people are afraid to talk about anything because of litigation. Games industry has gone a hard direction into everything is proprietary. The philosophy of O3DE has a power to change. Biggest open source project I have ever seen. Huge companies, but what it is saying is not only that you can do whatever you want with the source code, but that it moves what you are doing more toward your creative work. Games as competitive and secretive as the pharmaceutical industry. Historically been funneled down this path.

As an educator, interested in... this year has brought to a head the need for more open source acceptance in game development. If students are working from home, they need for instance adobe licenses, but we cannot get those to their personal computers from our lab. So we loosened requirements, use something photoshop-like or maya-like. How do we highlight the industry standard includes open source like Blender? Perhaps how should you migrate existing skills to Maya?

As a Masters in game design who uses open source tools: One thing that is often brought up is security, important in games. How do you keep secure? - having the code available makes it easier to spot bugs, we also fund security audits. One of the good things about software with an open source foundation includes having infrastructure to use.

Interested in how the pipeline of changes that companies make get back into the open source projects. I think long-term companies learn that it is painful to fork and become out of date. Good and bad, forking and using means they are using it. Long-term forks can be painful for those companies.

There's a lot of good, successful open source that has existed for years. Lots of the marketing around new O3DE Foundation are hearalding this as the savior of open source in games.  ...Trying to convince large companies that this is in their best interest is hard. Bringing a big umbrella should bring a lot of success. Blender is more isolated in its vision, though definitely sucessful. Want to build a community where there are multiple projects supported and funded, push for a commercial ecosystem. Either get paid by companies or make money for yourself.

From an industry perspective, having done games on many consoles. One of the big trends that will help is the meta-sphere meta-gaming aspect. Game developers are starting to realize that you need to partner with other companies to tackle this. This kind of collective idea, this environment wouldn't have been possible years ago.

Patents and making sure there's a level playing field is harder to accomplish from an indie perspective.

One of the biggest client-side challenges, is that consoles have NDAs. Been looking to break down walls on these. Huge barrier and makes things hard. ...let's start simple, try to get less NDAs and more deployment options.

Bringing this back around to the previous question on academic. Some students prefer something to be directly applicable in the industry, whichever tool has the most buy-in. Tools like Maya and Unity, students can use those but contributing to them is more staged and less open and accessible. At RIT we have our students take engine classes, to teach how to make changes at the lower level.

General question: cloud engineer that doesn't currently work in games. Culture around OSS, I often see that people around me don't understand how to contribute to open source. How do we advocate that change in an environment which is very secretive. How do you tackle the monumental problem of closed culture. Film, finance and banking, all used to be anti-open source. Training engineers and lawyers about open source practices. To add clarity, the OSS foundations provide a go-to place about the fears around open source. If there is fear about OSS, it's often about having millions of people throwing code at the wall without any standards.

There's also different perspectives you can work on for OSS culture. Introducing individual developers to open source projects can activate that hacker instinct, to not only use them but to contribute back to them can win them over. For leadership or management, it can be trickier... but you can point to existing success.

Historically in the game industry, need to have games ship to prove that products are worth using. Game developers primarily look at competition, for where to chase or one-up. Going to take showing by example. With game engines there has always been a risk that the engine goes in a way that doesn't suit you. This will be a relief for that pressure.



## Automated Testing Roundtable Day 2: Legacy
Andrew Fray, Lead Programmer at Roll7
### TL;DR: 
Start with something small that addresses a known problem, immediately squash flaky test bugs before they undermine testing culture.
### Summary
Effective communication is again called out as critical! Everyone had had pain with directly using input captures, and API-level calls are more stable (though humans then need to find the gaps where UI breaks). Tests which are 'flaky' need immediate investment to fix or de-automate before poisoning testing morale. Screenshot comparison is useful for low-level tests written by graphics engineers. Remote work has been a mixed bag of automation benefits and hurdles, better for those who are already distributed and/or cloud-based.

### Raw Notes
Today we deal with changing the culture of a code base which doesn't have automated testing, or has resistance to it.

#### Horror story at my org around bad solutions, where the scope of what you are testing grew and fell over
Automated input simulation, took this over. And old system that was about 10 years old, originally built by interns, and then sorta maintained. Nobody was particularly happy about it. Code was horrific, and nobody liked touching it.
* From the project side, our designers and programmers had to do a ton of work. Designers had to do a playthrough to be recorded, and then go back and annotate the file.
* Input playback is typically flaky. Have gone away from that and used more-contrived tests, obstacle course tests.
* Floating point error will typically exist in input recording. Instead can write an AI to play parts of game to make sure it still functions, or can be completed. Have humans do actual exploratory testing of the system. Keep automation budget as focused as you can.
* Throwing away tests and systems that aren't providing value is a useful path.
* For replays we don't use this much for tests, but we use replay for repeated performance tests. Had better consistency with API-level command replays, instead of directly using user input capture.
  * API test automation - what I've found is that it gives the maximum bang for your buck. Has the best stability, can replicate the experience the player has, build a solution for those using a play-mode test. Been running at Niantic for 1.5 years and never found a single false failure. Never seen that type of stability on devices.

#### Going to start a new game, tips on leaving a good legacy
What do I need to know? Don't let interns write your system? Keep things small, don't have long running tests, don't use input-based tests? Have a different set of tests users can run before submission, separate from those that run overnight? Will soon be setting up the ground work.
* O3DE automation documentation might be informative (as soon as I can actually get more docs shipped) we have examples of how we set our system up, and publicly provide all tests and jenkins scripts ...though Documentation is passive. Get buy in through discussion. When searching for requirements I find a two-pronged approach works best: both propose a feature roadmap and ask open-ended questions what they are looking for. RFC proposals may be a good start.
* Trying to redirect someone to documentation can be an antipattern. We have a tech-talk bootcamp recorded, placed on wiki to play back and pause. Scales not giving a talk constantly. Some people don't like video, some people don't read written words. Depends on audience and what they're looking for.
* Nobody is going to read the documentation so it doesn't matter (well, developers of the framework will probably do it). If you don't deliver the right thing, others will build that framework on top of it. Driving buy in and adoption is what is most important. Help a team achieve a goal. We do a lot of code review, helps understand the limitations of the framework and what to add to it. If you get the adoption and culture piece the framework will come along.
* Make sure you are interacting with your developers. Give them a taste of something they aren't used to, help them learn.
##### Takeaways: Wasn't what I was expecting to hear. Assumed I don't have to talk to people because things are automated, right? Sounds like it is the other way around!

#### Experience with tests that are flaky and unreliable, frameworks or mitigations
We've found that when we investigate, we do an extra investigation into the flaky tests. We find the root cause, and a bunch of the time it is a real problem to be fixed and not "flakiness"
* If you cannot fix a test, remove it and rewrite it. Warnings/orange/ignored-red makes devs stop caring about the tests. Huge problem because it becomes a low signal to noise.
* Usually a flaky test is too complicated of a scenario, you need to break it up.
* We moved away from any input recording toward an AI system that does the testing. Helps for us because everything is data-driven. Worked really well for us. Requires some work to make certain features work, to let the tests know where e.g. breaks in navmesh are.
* If you have a long-running test, it also doesn't help the purpose. Breaking basic validation apart from longer tests is important. End-to-end tests can take longer and run over night. Stress tests to find flakiness before adding into the daily run or runs in merge validation.
* There is a difference between flaky tests and okay-to-fail tests and tests that never fail, is that flaky tests will undermine testing culture. And no matter how good the test culture is, something can get through. Build that up, make sure tests are very trustworthy, hide it while you are debugging it. Keep the trust there.
* Waits need a condition to wait for, don't blindly wait and then assume! Callback or semaphore can help sync to event. Sometimes okay to add timeout/retry glue, for instance when dealing with network connections.

#### testing in mobile games
We were trying to build out automated testing in the game from various angles, half way through development. Was trying to do input playback, or have an AI pressing the buttons. Never got either fully working. Very UI-driven.
* Working to include automated testing with complex gestures, and work-in-progress. Have not found stability issues, but just to connect the dots - retry and other stuff, wait for element, there are some elements which play into it, some times there is google login and then that loads slowly. Those kind of things can play into it, but we have a very robust platform in Appium. Does provide retry mechanisms to see... after so many seconds just timeout that the page didn't load. Screenshots and videos can help.
* Got lucky with help from UI team, and creating direct engine hooks into the game API. Move the menu interactions into game logic instead of using controller input. Made it easier, no matter if the UI changed, could still work.
* In one GUI system could query what is available, but then changed GUI system to where this approach cannot be done anymore. All our tests broke!
##### Takeaways: Keep tests isolated and small, use lower level API as well as sanity tests. Don't rely on full playback tests at the UI level across the product.

#### Comparing screenshots
* On Sea of Theieves we primarily targeted this to the rendering team. We had tests where we took screenshots, cannot do pixel-to-pixel comparison. Didn't try to do pass-fail, would rank them by a perceptural diff algorithm. Give humans a direct scan for visual differences on whether there is an issue present. Have developers or QA look at the images.
  * Team would craft very specific almost-like-unit tests, and then take a picture which includes text that explains what the test criteria was. "the arrows on this wave should be green and pointing to the left"
* Can move camera around and grab screenshots with framerate and polycount information.
* Golden image comparison needs to be for low-level functionality, since it is so fragile.
  * We show side-by-side what pixels are different, from a rendering perspective. Use of this test is out of my scope however, graphics team would better know its inherent use.
* Having two versions of an art asset, one with every color in the palette. Terrible purple/pink texture to run through thousands of areas in minutes. Make sure shader is sane.
* Accessibility hooks from a screen reader may be useful

#### How have things changed being remote for a year?
Couldn't just have a machine running in the corner of the office. AWS machine would initially freak out on startup of tests due to not having a monitor! How have other people's automation changed?
* People complained about having machines in the office which they couldn't take home, such as PSVR headsets.
* Needing to be in the office for a hard reset was a pain
* All our stuff was already cloud based, so we didn't have that issue... in terms of the adoption, all of this happened during the pandemic. This was better because with a large team it's normalized that we can jump on a call. Communcaition improved






## Lessons Learned in Adapting the 'Sea of Thieves' Automated Testing Methodology to 'Minecraft'
Henry Golding, Principal Software Architect at Microsoft Mojang Studios
https://www.gdcvault.com/play/1027345/Lessons-Learned-in-Adapting-the
### TL;DR:
How to lead a large game team to adopt product-wide testing.
### Summary
This talk rocks, go watch it: https://www.gdcvault.com/play/1027345/Lessons-Learned-in-Adapting-the
For context, also watch its (possibly better) predecessor talk on Sea of Thieves https://www.gdcvault.com/play/1026042/Automated-Testing-of-Gameplay-Features

Games that constantly update require sophisticated Continuous Delivery (CD) systems that make sure they stay working and ready to deploy. Automated testing is a cornerstone of CD. You will need more than just small-scope tests of code, and the Sea of Thieves talk from a few years back shows a great example of this (Henry previously worked on this project at Rare). This approach was adapted to work with Minecraft, which is a significantly different game far past its launch (with a legacy of untested features). However both are open-world games with many interactions between different features.

Many devs start at fully-integrated tests trying to mock player behavior, which leads to very slow tests which produce many bugs but with little context or information. Unit-scope testing is a natural step for developers familiar with the low level code, and gives fast and reliable tests. However certain unit testing scenarios are impossible due to necessary and risky architecture refactor to isolate dependencies (even then could take ages to set up scenarios at a low level). Minecraft's answer, following Sea of Thieves, is to write semi-isolated unit-like tests in the game.

Whenever you transition how you test, have someone own driving the transition! Recommend targeting consistent, widespread use where everyone particpates in testing. Meet the code where it is today instead of blindly applying principles. Unblock as many approaches to testing as possible. First bias for adoption, and then scale up. Directly involve the developers by seeking and responding to feedback, don't just "drive" changes to their workflow. Leaders can stifle or accelerate change so make sure you have them onboard, else testing will not have schedule room to be prioritized.

### Raw Notes
Been in the industry about 12 years, even split between gameplay programming and automated testing. During my career the way games are released has changed a lot.  Burned on a CD to many patches, to games as a service. Games that update constantly.

Big problem that makes automated testing important. Games getting too big to test effectively. Constant releases get much more expensive to test on every release. Many studios looking for a different way.

On Sea of Thieves approached this through Continuous Delivery. This is about releasing when you want because you keep your test in a releaseable state. Automated testing is a cornerstone.

Every feature needs to be built with tests that make sure it still works. Great talk in 2019 about this in Sea of Thieves. One critical part is that gameplay developers wrote their own tests, most effective way to generate and sustain the test collateral.  Also standard practice just about everywhere else in teh software industry. When given the tools and training, game developers are quite good at it.

SoT vs Minecraft
New project VS decade mature game with many players
Designed testable VS not
Small team VS hundreds of devs ?????
Both are open worlds and hard to test

Goad: devs writing tests - how?
Can be hard, often fails
Successful on both SoT and Minecraft, let's talk common principles

Small team during transition

Principle: someone to own the transition
Culture change: tools/frameworks

Functional/integration testing
Common starting point for game developers. Have a tendency to focus on interacting with the game on the level of the player. The desire to automate manual tests.
Can be slow, unreliable, hard to write (indirect code access from high level) ...doesn't scale very well. Looking at test times in the tens of seconds or higher. Tons of unrelated code causing tests to fail, often hard to identify why the test failed. Many more points of failure in this test.

Unit tests
Natural step for programmers: fast, reliable, easy to write (direct code access) ...often impossible due to architecture! Isolating code from dependencies generally requires a big refactor of engine-level code. Even if we're familiar with engine code, often consider it too risky (especially without tests) to refactor engine code.

Test writing on Minecraft
Functional/integration were too slow/unreliable
Unit tests blocked by architecture
e.g. need whole game to test method on a component!

Principle: meet the code base where it is
Unblock unit testing for game code
Middle ground tests between unit and functional. "Actor tests" on sea of thieves
Try to satisfy the test dependency as simple as possible.

Test code in a unit test type of way, even though it is still integrated into the running engine. Made up 70% of tests on Sea of Thieves.

On Minecraft we call these Server Tests. Inspired by the Actor tests.
Call them this because the previous one executed on the client thread, and we decided this should execute on the server thread. Balance between easy to write, synchronous code, and keeps it not too flaky. Made up 35% of tests on Minecraft.

Principle: bias for adoption
Go for consistent, widespread use where more than just a few people are engaged with tests. 

RE-REVIEW THIS

First step was to acknowledge that the test framework wasn't working, insufficiently easy to write tests.

Start by taking on some technical debt to make test writing easier, paying down as adoption takes off.

Principle: Then scale
Adoption leads to lots of tests!
Speed up test execution: reuse expensive objects, deal with state leakage. Reuse objects, avoid reloading levels. Deal with state leakage downsides, which can accumulate.
Mitigate unreliability through auto-retry and quarantine/disable.
...reduces noise for developers, can track down problematic work later, without shutting down productivity immediately.

Principle: Involve developers
Seek and respond to feedback, don't just drive change
Early adopters want to help, will add features themselves once motivated. Faliciate their help, aim to transition ownership of frameworks/features. Changing adopter to evangelizer often occurs when their test catches a bug, since they feel the value of testing in a visceral personal way. Good to get as many devs to experience this as possible.
Facilitate this community building of test-knowledge in the dev organization
Having a pool of test-reviewers to review the code other others is useful

The goal is, over time, everyone acquires the skill to be in the group - then you don't need a separate group anymore.

Principle: Support from leadership
Leaders can stifle or accelerate change, both of these projects had support
Adoptions needs informed support, hard to give data on benefits of testing with it so rare in the industry
Stil difficult managing expectations, productivity may slow down to begin with, likely to provide long-term benefit for short-term slowdown. Takes time to reach critical mass of tests to provide broad benefit, not just local.
Give devs space to experiment, likely takes around a year to spread out this culture
Another major antipattern is to have separate tasks for tests. Good to have this be an inseparable part of the definition of done. Overscheduling in general is harmful, Actions speak louder than words and not giving tests space to be done is a problem.

Number of tests over time: unit, server/mid-level, and functional/integration tests. Functional tests slowly grew. Server tests grew rapidly. Unit tests more recently growing, likely showing that developers understand the value of more-unit-like tests now.

More bugs caught before manual testing! Top survey score for the test frameworks! Still a long way to go, challenges of scale. Still optimistic about direction.





## Interviewing for Design Roundtable Day 1
Casey Bradley, Game Designer at Traega Entertainment
### TL;DR:
Grab bag of advice on how interviewees and interviewers should prepare a for design interview.
### Summary
Design interviews are similar to technical interviews insofar as 1. cross-functional teammates should focus on communication and leadership skills 2. an expert in the field should evaluate design expertise. There can similarly be a catch-22 around buliding a culture which you don't already have, though one can work toward this by learning about the profession. It's important to examine your team composition and culture, to understand what gaps you have and questions which clarify how someone will fill that gap. Relevant previous talk on how to interview for design: https://www.youtube.com/watch?v=uUQKbowVsIE

A design portfolio does not always need to be in a specific tool/engine/genre as design skills can be applied to many problems. Though definitely cater your work to where you apply: demonstrate using anything the company is known to specialize in or exclusively use. Paper designs can be useful to bring to and interview (also a good medium for answering interview questions, along with whiteboard). Professionalism in design means listening to the right people, preparing a specific plan with failsafes, and communicating such that others help build the correct thing.

Some do's and don'ts were shared along with advice on interviewer/interviewee prep, but virtually none of this was specific to design interviews. One exception: when creating a game design portfolio, focus on the mechanics of game design and not the polish of graphical design.

### Raw Notes

#### How to build a protfolio when there isn't an established track record of working on a specific game or type of game. May not have the resources to copy or show that you understand a specific design space. Is there a more universal project that can show this off?
Tools like Unreal are available to experiment in. If you're outside of that space, other types of projects can still show the types of skills a company is looking for. You need to extract a micro-experience out of that. Be able to see a little clip which is punchy and attractive, that makes someone want to download a game.

Keep in mind that you can cater your work toward where you want to work. Cater to RPGs for Bethesda. Don't need more than one or two examples, not like 20.

Do your homework on the company you are applying to. Try to understand what is broken or the most popular, and be able to talk to it. If you want to do XYZ design, be able to show this. If you want to do system design, have an excel spreadsheet ready (even though it is a bit boring).

Is a youtube trailer a good grab-piece to put out? Single video across different projects.

You can do this on paper! Often better if interactive, but have been impressed with system designers that come with a paper design for a new unit or system. Allows us to evaluate their ability to communicate.

Professionalism in game design doesn't mean wearing a suit and tie, it means listening to the right people. Being prepared. Designer isn't an ambassador, they need to get others to help you build a thing.

#### How can we best interview for design when there isn't established design expertise or design culture? (perhaps a question on establishing design culture) Similar to AJ question about in-group bias.
https://www.youtube.com/watch?v=uUQKbowVsIE

Struggle with understanding our culture, so we can target the right type of people - then need to work on structuring questions to reveal people who have strength in areas you are weak in.

Bias and not having established culture: Recently started a new job, only a team of four designers right now. At leadership level trying to restructure interview to not have bias. Have culture interview which is not to ask design questions, then have designers ask design questions. At a previous place this worked well for a studio-wide culture, not necessarily an inclusive culture within the discipline.

Incorporate people from other disciplines, being able to work with those from other disciplines is an important daily activity.

#### What questions aren't appropriate to ask as an interviewee?
Candidate who asked as their first thing "what kind of snacks are in the breakroom" - this can send a signal that the work itself is not what you are interested in.
What to do when you exit an interview, reflecting on my own behavior does anything stick out to me? As much as they want you to be engaged about working there, generally turned off asking about things such as perks and how many hours a day people tend to work. Consider what you are asking, when you are asking it. First figure out whether the work is a good fit, before asking these more specific questions.
Other strange questions:
* How long after starting before I can add things to my portfolo
* Change subject to salary discussion, during a group interview with 10 people.
Do not tear down your prevous teammates, ability to collaborate is important! Focus on growth opportunity instead of past failure. Don't dwell on the dark places, talk about how you moved past them.

##### Questions **to** ask:
More risk in not asking enough questions! Engage with everyone you interview with. If you are interviewing with two people, talk to both of them!
Ask about processes and culture - figure out if company is a good fit for you

#### A lot of jobs ask for AAA shipped titles. What jobs should I look at to get the experience ball rolling?
Apply anyway even if you feel like you have the experience needed. (otherwise focus on acquiring experience)

#### What are portfolio trends you like or are tired of
Too much time not on design stuff in your portfolio, too much time on graphical polish
I have done peer review, I see things like 1-hour code project in Unreal and reused assets in RPGMaker, etc. Looking at those pieces can be difficult to separate that from what their own work is.
Personally want to see time spent on mechanics, and visual design not as important. Reusing our team's assets in a mod has done really well in the past.
A graduating class coming out of a school with 10 designers all working on the same project ...what is going to make you stand out above those other 9?

#### How to set yourself apart, what about the unreal kit where it's up to the designer to put it together
Analogy for soccer/football, having everyone playing on the same platform everyone on the same stage, after we take apart circumstances... now that they are on an equal playing field, it allows you to zero in on how they compare.

"Day of death" interviewing experiences mostly get you people who interview well, not necessarily highly skilled. Don't have people be there dawn to dusk. Treat them like a human being.
Trying to shorten the interview loops, risks cutting into more days of interviewing. Everyone needs to be trained at interviewing, too.


## Technical Issues in Tools Development Roundtable Day 2: Pipeline
Geoff Evans
### TL;DR:
CI Pipelines are an incredibly useful and expensive magic: log metrics, reuse and share tools wherever possible, and be careful about licenses.
### Summary
Most of this discussion centered around how tools in a pipeline get built, maintained, and shared across projects and environments. It's extremely common to solve similar problems in different ways across different teams. While the activity of engineering a solution can lead to innovation, constantly re-solving the same problem across multiple projects is very inefficient. When you have multiple teams tackling similar problems, have engineers across teams pair to solve something. If there is reluctance to use an existing tool, remind a team about maintenance costs.

Often NDAs mean only a larger company and/or game-engine maintainer can try to direct deduplication efforts, though one attractive answer is open source tools. Though be very careful about examining the licenses when you use something you don't maintain, for instance PyCrypto states "it is a violation of international treaty to distribute this outside of the USA" ...! While there's some agreement that maintaing a central tools team can take burdern off other teams, little wisdom was shared about how this team should operate.

There were short discussions on what makes game CI unique from other applications (scope of content and velocity of change), as well as general scaling issues with pipeline latency and space. Unpacking these complex subjects did not lead to concrete takeaways: record metrics to understand where time and money are disappearing, and engineer solutions as you find problems.

### Raw Notes
Today is about build systems, cacheing, getting data into the game

#### What is the difference between game CI and other CI?
There are useful talks for Call of Duty as well as Jenkins in the GDC vault. Rare's adoption of CI https://www.gdcvault.com/play/1025028/Adopting-Continuous
WPF standard CI tools. Using Git instead of P4 is a bit different. Most game studios have people working in the same stream. Running tests before submission on their local machines.

#### Sharing tools and patterns between teams or projects, where is the line between a tool being "reusable" and "not built here"
Everyone has bias toward using their own tools. (need reasons and inroads to use another tool)

I work on a central tools team. Try to identify these places where things are duplicated. There's a lot of the organization of the company themself, have the leads around on a regular basis of understanding what can be shared between multiple games teams. Getting the right people regularly speaking, to break down barriers. Understand where there is duplication.

Where I've seen this go really wrong is where the decision makers on a particular project aren't doing an evaluation with the normal tools developers. E.G. a GUI tool did an evaluation and a team made a tool based on their own evaluation. Making sure that there's a buy in with the decision makers with a team.

+1 to central team that takes a good tool from a studio and says they will support it, and have a central tool that people will invest in and support. Executive level decision to not waste resources and cut costs by reusing a tool.

Biggest thing is when you have people trying to develop their own tool, and they perhaps shouldn't, and you can remind them of maintainence.  Then a few months later the tool rots. While developing, yet another team starts working on a new one. Perhaps these tools should just be open source in the first place?

Keep it simple to reduce maintainence! (and initial investment)

Try to negotiate a problem area with someone, carve out a space where you think there is a problem, and whether making something with a general surface area would they adopt it?  Negotiate taking maintenance burden off someone.

When working as part of central team, trying to build tools for a team - you are building something and someone else is already building something similar. Clash in duplication, even if you announce there will be people who won't know about it.

#### Hardware problems around latency, minimize technical issues around huge builds and running out of space.
Large assets and long builds are a problem. Is "get another build machine" the solution? Use a threadripper, does that help? Use RAID NVME drives?

Long process between local machine under a desk to rack-mounted gear. Depends on how the assets gets compressed. We check everything into P4. On the hardware side of things, depending on the problem, we build out compressed textures to EC2. Found that asset builds are really heavy so metal machines are better there. Using P4 edge servers, lots of things to optimize the scalability and turnaround time. We load our own assets off disk, so we have a large asset server that's independent from code repo. Code repo at Monolith spans from 1996 to current through all projects.

Multi-pronged problem teasing out how to optimize the build. Scheduler process doing resource acquisitions, that could be one potential bottleneck. Once you've married a job to a worker, you've got coalescing local state to be ready for its logic. Resource acquisition side is important. Scrape windows performance counters minute by minute to be able to monitor the utilization. Start to see the bones of where the bottleneck is.  Insidious thing about builds is that there's a lot of time spent doing disparate things. How long it takes to upload a result, moving identical data and dedupe over the wire is important. Keep local cache of chunks of files. Find where the dollars are evaporating.

Human Communication is an NP-Complete problem, iterative solutions are important

Have people pair to solve something, when you identify that they are trying to work on similar things.

Design documents to communicate requirements and how they solve their problems. Sometimes it becomes separate solutions that only look slightly similar. Might need to get humans face-to-face to understand whether problems are the same.

Issue with bias is that, resolving that friction is about building a culture of recognizing we are biased to our own solutions. Ability to recognize that one solution is better, separate self from what you are doing. Be able to do objective analysis. Talk to your users!


#### Using open source in your pipeline

Major difference is that there are no intermediate tools that you need to take care of.

All of blender scripts need to be published under GPL. Might be legal issues working, importing stuff from blender.

We try to be as DCC agnostic as we can, blender was our proof of concept. Put fear through a lot of people in the tech art community.  Can design software in a safe way for e.g. RenderMan.

GPL infectuous through scripts? May want to talk to legal. A lot of this has not been tested in a court. Redistributing and selling is where the rubber meets the road. The moment you give it to an outsourcing studio is where things might get more complication.

I like working with tools vs with the game, because you can just release internally. If you write under GPL you aren't required to publish under GPL, just license under it. Perhaps you need terms of service to give to outsourcing studio, you could get around 'distributing' it or selling it.

PyCrypto "it is a violation of international treaty to distribute this outside of the USA" !!!

Sticky situation for in-house tools, where someone in bizdev thought it would be a good idea to start licensing our build and content tools after they were already built. This was a nightmare to go through and do.

If you think you're using a GPL thing, it's not about whether you're charging for it. You can be asked to open source your code. Be very cautious that they are very risky licenses.

#### Developing tools for many different devices, porting tools across target platforms
Have a bunch of different operating systems which need to interact within the CI pipeline. Perhaps WSL can help?

We run into similar issues in our standard software, we containerize our code. Doesn't help for perf evaluations.

Unreal has gauntlet and a bunch of plugins, but once you start running deep into them, there's a lot of different things that you can't do at a lower level.

Ran into a similar problem for testing multiple players at once. Used unity to produce a build for each different platform, and then use that produced build to run on different platforms.  Cannot be one answer for all the problems listed here.

For testing multiple platforms, at Infinity Ward we have a wrapper framework with special things such as wanting to boot the game at a given level.  Wrapper that does common things across multiple platforms, when you want to set up or tear down, go from there.

Cross-platform scripting language to cross-platform compliation, etc. Few silver bullets to address this problem.


## Final Fantasy VII Remake: Automating Quality Assurance and the Tools for the Future
Fabien Gravot, Lead AI Engineer at SQUARE-ENIX
### TL;DR:
The complexities of making automated input-replay tests function: more bugs were found, though neither time nor money was saved.
### Summary
The speaker's tiny team of engineers was requested to make an input record-and-replay system to automatically play through levels in FFVIIR, well into the game's production cycle. Many different features were combined to to make the tests more stable, including preexisting exploratory testing features the team had been working on. These tests exist as an external system which sends events into the game, as well as reads events logged out of the game. This external system synchronizes its inputs to the stream of game events such as player collisions, area or timed interaction contexts becoming ready, cutscene start/stop events, etc. Building an entirely separate AI controller from the game prevents needing to modify the game's source or impact game performance, and may be more reusable on other projects. When events were not synchronizing, the fallback AI behavior was to pathfind toward the known goal, or to search for unexplored area. This would be allowed to happen up to a threshold before giving up and marking the test as a failure. The results of this system were groomed by its maintaining team, to try to isolate bugs and route them to the proper team.

This level-playing system was not delivered early enough to help offload manual testing effort from the QA team, and it is unclear whether its results were stable enough to provide early warning notification to the development team. While I remain interested in the potential of the record-and-playback and believe exploratory testing is extremely valuable, these automated approaches are not ready to replace human effort. I suspect writing smaller in-game automated testing scenarios would pair well with humans focusing on exploratory testing and gameplay (user acceptance) tests.

### Raw Notes
Most of the developments have been done at the end of the project, only part of it has been used before the master.

Replay system, its synchroziation and workflow
Explorations through the game

Game QA is evolving:
More content, more interactions, greater freedom of play styles, post content services and support ==> increasing QA cost!

Can improve reporting tools: automatic data entry of bug reports, automatic crash/assert reports, telemetry data
Can try to automate playing the game with tools: with scripting, or with replaying user data, or exploration

In replay, there are things you can do without changing the code, and things you can do only with code changes. Cannot rely on tests changing code in the master that ships.

Replay system has a runner/launcher UI but can be started by CLI. Starts two processes, one is a automation server to supervise and control the game, another is a watchdog of platform-dependent code to start and monitor the game.

Part of the game code is Auto-QA game specific. Part of its data is generic game state, position, level ID, velocity/orientation, game time, utc time, real time, frames count, action/event keypress or pad input, game state stack of ID's

Can also store synchronization flags. We won't discuss all of the types, but let's talk about one. Where you are supposed to find a game state at a certain point, where you need to be at a specific position to start.  Begin a timeout, and then continue to do some play to make sure you can get to a location. If you reach it, you can synchronize game script after that point.

For a relocatable start, could be something near a location where it is allowed to start. If we start to see syncronizations, then compute an end-timeout based on the original timeout.

Skippables (cutscenes) appearring which are new since the replay was recorded.

Startables (interactions) that could be opted into, want to be sure to opt into the ones recorded in the script.

Optional flags, try to do parts of a script, and skip if they don't appear.

Pathfinding in the server, create a 3D grid map and use both previously recorded and a replay map. Detect wall when motion doesn't match the input commands. If we are lost/blocked, try to pathfind from current location back toward recorded path. Fall back on in-game pathfinding if we are still stuck, but don't assume the ingame navmesh is valid.

Record data from PC, save the replay data and save to Jenkins. Save test results in database, use some results for monitoring data, some go to QA for bug classification, some go to automation specialist for debugging test system.

Replay Advantages:
Finds rare bugs, logic and multi-threaded errors, many occur less than 1% of the time.
Tests level traversal issues, 300 tests per day.
Tests large code changes for crashes
Test games with bad frame rates
Able to perform tests 24/7, including soak tests
Disadvantages:
Mostly tests a recorded path, doesn't go outside this.

Exploration Advantages:
Finds bugs outside traditional paths
Capture perf analysis
Check sollision
Disadvantrages:
Cannot test all possibilites, cannot finish certain levels or tasks

So we mixed both! First explore only, then as we start to fill up the map, use pathfinding to fill holes in the map. When we feel all the walls, we try to find not yet explored areas. If we have a battle occur, we try to synchronize with the replay data if there is one, then continue exploration. Then go back to exploration.

Conclusion:
Advantages:
Automated repetitive testing
Share development costs through several projects with generic game state, navigation systems that are not game-specific
Disadvantages:
Cannot perform qualatative QA for e.g. graphical issues
Future work:
Extract video/screenshot for QA to check
Explore on other actions: menu, minigame
Tools to find large changes in level collisions

Q&A

We have a system at Jackbox where we automate the drawing and people testing. Always when we do this we run into bugs we cannot reproduce by hand, since automation works in a different way. Do you run into bugs you cannot reproduce?
Depends on the bugs, some we cannot reproduce because they are very rare. If it is a crash bug or a logic bug, we can let the system go to this state and have someone analyze it. If is an issue in the test system, we have a classification system where we can ignore certain bugs based on their frequency. When we can, we try to fix our bugs, depends on the frequency.

You mention that the test script replay tries to synchronize. What is it synchronizing with? I assume these are essentially a series of log events or profiling data?
Part that is kind of game specific, try to extract game state and give things ID's. Give positions and also game events. Have a specific ID for things that are already in.  Not as strict about cutscenes but they are good for , strict for button presses like opening a door (fail on those).

Mostly seemed to use replays for battle system and exploration, is there something for the cutscenes? Do people need to look at them to understand what issues there are there?
Every day on checkins we do replay and record what happened, and export cutscenes that have been replayed to video, can check on this when they need to. We cannot automatically verify the cutscenes, but we can replay them.

For the visual representation of the test data that appears in game, how early was this added? (game development progress / test tool progress)
Debug tools were added quite a ways after the start of development. Part of plugin code, does not need to be added on the game side. Server has a world map, and sends part of it to the gameplay end to be able to display it. Program with running out of memory and cpu, so we have the ability to disable it.

Did you find that the tools you were automating with, were the tools changing out from under you? Did you have to tweak the game constantly as the game was getting more assets?
We have sometimes to re-record things due to a data change. Tried to make the system robust enough, but if you start with a menu system that changes to a tree of things, we cannot easily serve that. Even the default values in a menu can be tricky. Need to have more game-side knowledge to see the difference between buttons. We won't know when the design changes, and since we are part of a replay we know where the data is failed, have it replay up to theis point before starting a new capture from the user.

Is there any data you can share on stability, speed, or cost savings?
Cannot give hard data.  To be honest, we are not yet saving money with this. Some savings between game systems, it was easy to add PS5 support since this is external. Initial cost on this was higher than what we saved, still trying to improve our tools so anyone in QA can use this.  Hope to soon be a cost savings.
Currently have some performance issues that we want to solve, nothing really hard, but this is the first prototype. Sometimes have a slow FPS from using a lot of debug tools, since the system is already slowing down from time to time. Bugs that are physics bugs on a specific system or on a specific level of FPS.
Stability for example, we may get lost or have steering issues, may turn a corner and not be able to go out. Not a bug, we hope to improve it.  As long as the issue is not too big, we don't cut a bug for it.

Failed 1 out of every 5 replays something like that.

From time to time it is not good to press buttons too fast.

During the chat, someone mentioned that consoles, how does this work with actual physical controllers? People do wierd things with actual controllers when they play.
Only FF7R that I can talk about, of course PS4 and 5, and windows is the most used development platform. Good to have stability across these, but our main target is of primary concern. Did we have problems? For example replay from PS4 would work on PS5, problems when the opposite was used. PS5 has higher framerate, and could add problems going the other way.

What is the team size, and how long did it take you to get to a functional solution.
Cannot say the specific number, but team was quite small, less than ten. Currently not yet confident enough to send this SDK out to anybody, when we have something we can say can be reused by projects, I can say we are fairly sure that is the case but we need to do more work to become more confident with this.
We started as a research project, with explorations of what we can do. The team of FF wanted replays though, not exploration. Took us less than six months to have this working.

Can you say how you got this greenlit? Was started as a company-wide desire to get a better path. We are infact, I am working on advanced tech division, trying to get new tech into all projects. Generally I am working on navigation system. Bsically we have some link with other projects, so when the project was at the end, we said we needed to have more QA, and then we developed this with them.  Quite difficult as this was a new project, we had to prove ourself, and they didn't have extra time to help us.

Solving input simulation, were you recording it into another system and then replaying into another tool?
We have an override of inputs for motions and camera, record not only the input but the positions, speed of actor etc.
Tried to use replay data as long as we are synchronized, as long as we  had similar locations, when we go a little bit outside, have a controller that tries to use another set of input. Simplifies control.
In the game state, if we need it to have input, we can synchronize this with positions. When you are breaking crates, those need to be done at specific points precisely. Not just for input, can be for other game events.

How involved was the QA team in developing the tests you were automating? Were they involved or was it all you?
FF7R QA was not involve in this much. We are working more on our tools to be better to get more UI and be more accessible to QA.
Sometimes if we are not sure if something is a bug, would ask QA to do a check.

Did you use the same system for the minigames, or did you need to do other things by hand. There are many variations of gameplay in this game.
For minigames we had some, depending on the game. Very simple ones based on sync, it was a common thing we could trigger for with X event do Y. For something can be more tricky, some games are being skipped, like the dart ones. Other research tried to look into ML, but it is not yet there.



## Better Live Game Releases with Persona Based Testing
Nicholas Joebgen, Senior Quality Assurance Analyst at Pocket Gems
### TL;DR:
Rely on user personas to focus scenario-based testing where it matters, and reduce focus on low-yield regression tests.
### Summary
This talk is the story of one studio refocusing its testing approach to persona-based test plans, and away from regression of specific cases. Everything had been going fine through multiple content updates, though the per-feature test cases grew near 500 hours to complete. However, one day there started to be rampant failures which were now being missed by this massive suite of manual tests. It was deduced that the game was being tested in the same way as it had in its initial release, despite the game and its users changing drastically.

To adapt to their testing reality, the specific regression cases were replaced with a set of player personas to inform how a build should be tested. This included rules such as not using cheats or fabricated accounts, such that the actual player experience is tested, and switching between device platforms during testing. To save budget, they cut straight over to the new test approach without running it side-by-side to compare with their existing approach.

While the talk doesn't mention this, user personas are one method to create scenario-based testing. Scenario-based is an increasingly popular way to optimize where testing resources are spent, in part because it focuses on a human's superior ability to perform exploratory and ad-hoc testing.

### Raw Notes
Why we needed a new solution for release testing, how it was set up, and what the results were.

Mobile game that already launched, six years old and one of the top 4K games to this day. Grown and evolved into almost a completely new game over time. At the heart are tens of thousands of players that still log in every day. Players have also evolved with the game.

We have prerelease testing with onsite analysts and offsite testers. Have been able to keep a high level of quality with each releases, prerelease candidate and release candidate builds. To ensure running properly, had a feature control test pass for every feature individually. Ballooned into a suite which takes 50 testers worth of work days days to complete!

Has been working well for us, no fires, and no emergencies! Until two years ago, in 2019 there was a crisis arising, which other companies started seeing: client update releases started being unstable. Players would find show stopping bugs, requiring to fix and release. Costly!

We'd immediately ask why we didn't find it preprealse or in release candidate testing. Our eyes fell to the test pass, since we weren't finding the blockers. Soon enough it hit me, our game and players had evolved over time. Our release testing was the exact same thing that it was since the original release.

We never levelled up our testing to go with it. Could go back and update all our test cases to update this. We also had a huge problem with the existing test pass being too large, changing all the cases and adding more cases wasn't a good solution.  We needed a new system, a new strategy. System independent of cases for each feature.

Our answer was starting at me, our players. Though talking with them, we've witnessed that they fall into archetypes. All engage with the same game but for different purposes. What if we play the game like they do?

Let's test the same flows that they use. Test the game like they play the game. If done right, it should look like we've invite players into our development server to test our upcoming build. This asks our testers to, for the duration of the test, act in a certain role throughout the test pass. Break into 45-minute sessions, and then take a break. Around half way through the cycle, switch between OS's and check whether swapping between android and iOS is still supported. Different persona has multiple user scenarios, goals and tasks that make sense for that persona. Sometimes need to interact with other testers with the same persona, or a different persona.

To step into these accounts, we create new personas as a one time investment per test run. Don't provide debug hooks to the testers to hack the state, just play like a player would.

We now keep our same two passes, but we run them at different points. Pre-RC pass has around ten testers, and runs for about three days. Don't allow it to increase over time, just create a snapshot of what typical game play looks like.

When ready for approval, we run a heavier RC pass.  Use different amounts of personas to snapshot what the first hour or so of gameplay is going to look like.  Things that players are going to test when starting up the game.

Building the personas for testing
Need to document archetypes, survey was sent out to random players identified by marketing team asking specific questions, questions would player important role in helping establish archetypes. Marketing team was already planning this type of survey for other reasons. Cannot share the questions due to NDA's.

Our personas:
Harmonizer, social butterfly, looks to do things with other players
Progressor, in-game collector, wants to see and have everything
Organizer, creating social experiences, coordinating large team efforts, using third party tools
Aspirant, desire to be seen as the most powerful, driven by competition, acquire and train the strongest content for their playstyle
Tourist, new installs just learning the game, likely found from advertisement, learns about the game will likely become another archetype

Test scenarios
Set goals instead of specific yes/no cases to verify, test similar to how a real player would reach that goal
We break this into normal workflows and trying-to-break-the-game workflows
Separate both normal scenarios and upgrade scenarios

Create supporting documentation with a breakdown of each persona
Create flowcharts for persona, and what sort of actions they might take. Not step-by-step instructions, just a guide. Not a full test, a high-level understanding.
Create a matrix for all the features in the game. Useful for us as analysts to find gaps, but not important to measure a tester's performance or effectiveness.

Persona testing guidelines
No in-game cheats, all currency acquired the same way as a player would
Identify game session lengths for an average user
Use the same account for each persona, have that account switch between functional platforms

First trial run
Set up call with test leads to go over the new process. Need to have that understanding to step away from rigorous test processes
We switched straight over instead of running both normal cases and personas at the same time. Not necessarily recommended, but it worked for us

We were able to eliminate "not a bug" and "will not fix" tickets. Found an overall decrease in tickets, but the bugs that were found and filed were more critical and useful to get fast fixes.

First client release yielded no high severit issues, we continued using it throughout 2020 on a trial basis. Happy to say that this had better results than expected. We found that players reported bugs have decreased by 30%, and an increase in bugs being caught before releasing.

Internally, we sent out a survey to all the testers. 90% of testers preferred the documentation. Testers initially a bit uncomfortable with not having a define case.

Still run our old RC workflow once per quarter, but it mostly provides new edge-cases.


## Automated Testing Roundtable Day 3: Implementation
Andrew Fray, Lead Programmer at Roll7
### TL;DR: 
Configuration-as-code minimizes maintenance. Start small but log aggressively. Talk with console manufacturers about legally farming tests onto their devices.
### Summary
These are hard topics that bump into hardware management, test prioritization, risk management, teaching expertise, and legal/secrecy issues. Despite the discussion there are few answers, mostly anecdote and reference. Teams are likely to create their own scripts, but should try to start with the most updated versions of off-the-shelf middleware.

### Raw Notes
Today we talk about how to build what you need: off the shelf tools, custom scripts, etc.

#### What systems do you use? Jenkins?
* Have used GitLab, which seems quite stable. Has worked fairly well, working on getting that parallelized.
* Currently on Jenkins migrating to TeamCity, part of this is a reliability improvement. Dockerizing everything, brought on a person specifically for this. I wasn't aware of many of the tech choices before this new hire mentioned them, a learning experience for all of us.
* Have mostly seen Jenkins Used, some AWS step functions. Seen a lot with Jenkins that most people are using Jenkins in ancient ways.
  * Jenkinsfiles and BlueOcean can help a lot--use the modern improvements!
    * Configuration as code (JenkinsFiles) checked in and versioned is quite useful.
    * BlueOcean is about UI, it makes Jenkins a better and more legible dashbaord.
* When there is a Jenkins-bug, if anything in Groovy fails, it is super painful. Almost nobody has expertise in the Grooby JVM environment.
* Have seen Jenkins server failures related to I/O when scaling out simultaneous builds, for which you can only temporarily work around by buying bigger hardware. Start saving logs to files and uploading those files to a datastore, and *not* serializing all log output back from child jobs to main server in a dashboard. Just provide a URL to the saved logs. Users looking at logs and statuses in Jenkins can exacerbate an I/O bottleneck.

#### For those with NDAs, are you using public device farms or on-premises? Safety considerations?
One game team won't know what the other team is working on, until the other team discloses that.  Having hundreds of phones, etc, isn't feasible locally before launch, especially working from home.
* Look into the TOS of any separate device farm. Be careful and include legal counsel when evaluating a device cloud, in terms of what you can do and what you could be liable for - especially with NDAs in place.
* Due diligence with whoever you choose. Few of us have the resources to effectively manage an in-house device farm. Could make an argument that it is more secure to use an external farm that specializes in that! It's likely a different story if your company also owns a device provider service.
* We have seen someone else's app launch at least once on our provider! (security risk, secrecy risk)
* AWS device farm suggests a lot of this is on you to handle security properly, likely need a lot of training to use properly.
* Public cloud can give you a lot of options, but human error on your own usage is still possible. Have seen public cases where data leaked due to misconfiguration.

#### Complexity with amount of devices - XBox, Playstation device farm.
There's a bunch of solution for farms, but they only provide public products for the devices. What do people do for consoles?
* Frankly the supply for devkits is relatively low, especially for smaller production houses. Usually devices are leased on contract, and by the time six years are up there's no reason to have a farm for it anymore. Mostly it's supply and leasing problems, most of the leases say it's only for your group. Lots of legalese!
* MS might be making XBox available in the cloud: https://www.onmsft.com/news/heres-a-closer-look-at-the-hardware-behind-project-xcloud
  * PSOnline might have built something similar for their online play service?
* At Rebellion we have an entire wall of devices that we can't do anything with anymore! Licensing issue - not actually ours, so we can't destroy/discard them.
* Overseeing people who are testing on devkits in the office, there's a ton of issues that occur with things going back and forth.

#### New drivers for GPUs, etc, how do you test all the variation?
* From Monolith's perspective, we contact ATi and Intel and nVidia constantly to talk with their driver team. They usually have developer reps which contact you, and can help you find an issue and give direct engineering support.
* Surface as much driver information as possible. When it goes out to customers, log driver numbers to take back to companies, to point to issues with certain versions of drivers.
* Most places cannot afford to test on a lot of variants.  Need to be able to trust that the driver vendors will fix certain issues in the wild.
* Not efficient to release and then have to ask driver-author for help and repro steps, or when they ask for them. Crash information pointing to a specific driver can help, if you have a lot of data you can find a pattern. Still a post-process and painful workflow.

#### Introducing testing culture to new teammates
Who owns writing the tests? Who allocates time for writing automation?
* Very rare that someone can get a framework adopted. Leadership support is important. If QA is doing both manual and automated testing, how do you load balance that work? All of this comes from getting stated on test automation.
* Useful book: https://www.amazon.com/Flexible-Reliable-Software-Development-Textbooks/dp/1420093622
* The way I approached this, even though as a lead I could pull rank and demand testing, is that we added regression tests around certain bugs. Slowly start building up the scaffolding.  Dev team owning those and writing those.
* O3DE has the individual feature-areas (Special Interest Groups) maintain their own tests. This removes a communication barrier between teams about what tests are important, and which are worth maintaining. Separate small team helps build and standardize test tools, and fix bugs other teams report.
* Having a good horror story about things not going well is instructive. Stories/drama can speak very well to someone unfamiliar with a subject. Many GDC talks have good stories!
* I have found that, often when there is a testing framework there are only trivial examples.  Already having the crucial parts of the game available to be tested, this is important for the game developers to onboard and care about the framework.
* Curious - are people doing test driven development? (No, not really anyone! Though most agree that it can be a useful design tool.)



## Technical Issues in Tools Development Roundtable Day 3: Workflow
Geoff Evans, Tools Engineer at Epic Games
### TL;DR:
Good tool workflows are both extremely valuable and difficult (expensive) to deliver, with no silver bullet.
### Summary
Unsurprisingly this tools-workflow roundtable focused on UX. Some larger teams in the industry have a team dedicated to Tools UX workflows, but even then it primarily seems to be a virtual/extracurricular team of enthusiasts and not a dedicated role. Important to use team's existing shared vocabulary, and not new jargon. Some tools have an obvious savings of X hours per week and are an easy business-case sell, while other important features are about more nebulous or hard-to-estimate benefits like reducing friction. Number of clicks is a bad estimate for workflow complexity: https://www.uxbooth.com/articles/stop-counting-clicks/

Collecting metrics from your tools will help you understand how users actually use them, but take care on collecting useful metrics and not a wall of noisy events. (also be careful about misusing data e.g. removing underused buttons or options: some are critically important to edge cases, some are set-dressing that helps the user feel comfortable using the toolset to create content) Quarterly user surveys is a good start for collecting data. Having customers enumerate their common workflows can help focus your work.

There was some anxiety about deprecation, which is a tough problem. Habits are hard to change, and nag-ware doesn't always fix this. However do not overestimate the need to update or discard a tool, understand the actual value you're getting before investing in a change.

Of the devs in attendance many program UIs via C#/.Net Core, quite a few use C/C++, and virtually none use Python or other languages. For UI frameworks many use WPF, some use Qt, and almost nobody uses others such as WinForms, ImGui, MFC.

### Raw Notes
Useful talks: Lessons in Stability by Titus Winters from CppCon 2015: https://www.youtube.com/watch?v=zW-i9eVGU_k
Also Emotional Systemic Facial of 'Last of Us Part II' and "Swiping on the Six Strings": Crafting an Interactive Guitar in 'The Last of Us: Part II' ~ not pushing TLoU2 or anything but both were great talks and finally Classic Game Postmortem: 'Star Wars Galaxies'

####  Debug tools, those can send information from one group to another group. Binding of debug data.
Tools tutorials days has a good talk by Amy Phillips from Media Molecule: https://www.youtube.com/watch?v=ro0pNuoGONU
Send information to Jira via REST, post stuff into the bug. Can hold game state within attachments to be able to teleport somehwere into the game

In our case we have debug tools overlaid on the window. Communication is tough, that when you see a problem you should have a certain tool open... saving this visual data relies on image compression, not great for saving.  Want to know where specific state was wrong. Acceptance level issues, which aren't necessarily assert-level.

We have a graphical log. Engineers write the hooks that send the data out, drives developer engagement with the system to get more data on their area. Have both sides of the coin.

Determinism is very important to support replays. Test for determinism.

Our games have a constant recording of the stream of data coming out of the game, and can get the last N seconds of data leading up to a bug. We have one debugger for animation system, another for another graphics system, set up to record their own data stream.


#### What UI libraries like ImGui or web/WPF type tools? (poll of 27 people in attendance)
C/C++ for tools ~1/3
C## or .net core ~1/2
Python ~1/5
Build system uses google ninja, with python

MFC 1 person
WinForms 2 people
WPF ~1/3
Dear ImGui 1 person
Qt is ~1/5


#### What do people do to judge workflow quality
At bungie created a team to address this specific topic. A voluntary group that meets part-time, hiring a full time position to get workflows efficient and users happy. Conduct survey about pain points. Goal of team is understanding where to put engineering resources. A few years ago did a blast e-mail to get "the first three words that pop into your head when you think of our tools" created an interesting word cloud.

Usability, importance of familiarity. Look at what is already being used, since they know how it works. Gives shared vocab when discussing issues.

Not a lot of teams gather quantative data from tools. Does pay dividends, not to make decisions based on the data, but to narrow down who you should sit with to work and understand their qualitative feedback. "Swizzle the matrix" probably won't speak to an artist. Get up from desk, or stream desktop. SILENTLY watch them. Helps prioritize.

Telemetry system that can record basic events in your tools can provide great information. If you don't have time in your team's schedule, can be a great intern project. From one tool, found that almost 50% of users did not use shortcut to save, surprised us!

Great talk from Remedy a few years ago, with an event-system back end where they mostly know what a user was doing. Could more easily clarify certain issues.

Knowing what data to send to telemetry is hard. Some easy to send data can just create noise.

When you are making a business case for a tool, you often can show it will save X hours across Y time. Some tools are about reducing friction, harder to put a number to. Can't wait to be back in office to better identify these improvements.

We found creating quarterly surveys led to a good starting point. Short survey, if scores weaken or fail to improve, then reach out for more specific data.

Telemetry is good for know what is or is not being used. Challenging to contextualize the user's motive or goal. Has anyone got a way to get that contextual data, of what their workflow is?

Tools Tutorial Day: Workflow Driven Tools Design: https://www.gdcvault.com/play/1025289/Tools-Tutorial-Day-Workflow-Driven

How do you enumerate the workflows you want/need to work on? How many is it?
Group is working on this, have a definition of workflow. Series of tasks meant to achieve a specific goal, focused this to using our specific tools and not the third-party tools. Matter of scope, there's a lighting workflow, but there are multiple small workflows that contribute to this. We have 6-7 major workflows.

Frostbite at EA, serve a lot of game teams. Have a similar definition of workflow. UX team recently started gathering info on common workflows. Go to content communities (disciplines) and have them enumerate their 10 most common things in the engine, give a demonstration of what that looks like.

At Amazon we index as much as possible on customer feedback, reduce pain and improve satisfaction. If you're exploring the space and assuming where you can improve things, might end up modifying a workflow that users didn't want changed or add a new option they don't want to adopt.

Searching in the wiki you may find already documented workflows.

Perhaps you can use Jira to collect information. While this will likely be noisy, you can search through descriptions. Tends to contain the master TODO list across the company. May be able to refocus on known tasks/goals.

I think there are objective criteria for certain parts of workflows. Things like performance, responsiveness, how many steps from one reference point to another, to a new observable result.

Make sure that when people say "number of steps" they don't hear "number of clicks" not necessarily a measurement of how bad/inefficient it is. Motor load is something the brain handles easily, cognitive load is more difficult. https://www.uxbooth.com/articles/stop-counting-clicks/ Cognitive > Visual > Motor

A good workflow is something that lets the person do what they came to this company to do. Anything that gets in the way is what causes friction. "Get out of the way and let me do my thing!" for instance "how can we make this configuration tool better" was met with "we don't want to have to use a configuration tool"


#### How to sunset tools

So much of this has spoke to work I've done with corporate communication. Labs of scientists, sometimes both will be inventing the same wheel. Really beautiful to see where we are with telemetry and the integration of tools. In the army we did a partnership with Epic, to use America's Army instead of an in-house training tool. Were trying to revise 1100 page website. All these people saying we have to sunset this, due to data saying people were navigating through the site and then dropping off. Was not useful or actionable data. Would be like saying "reduce the number of places you can go in Red Dead" (because some of them are less captivating)

Added a chatbot for what people were searching for. When scientists are working on something, documenting everything is natural.

Trouble getting people to move, people still want to use old ones. Even when support has been dropped. Move feature requests to the new tool. Go slowly if features are workflow-critical.

Don't discount the existing productivity, don't nuke the tool unless it is insecure or has some legal issue.

Shareware-style nag screens. Sometimes have to kick people to get them to move.


## Using Non-Functional Testing to Guide Game Development (Presented by HeadSpin)
Brien Colwell, CTO of HeadSpin (a mobile device test farm)
https://www.youtube.com/watch?v=8mkql3sQ8MI
### TL;DR:
Collect metrics about your app performance, and alarm when they break a sane threshold.
### Summary
For a sponsored talk, this was moderately less of a product pitch than expected. It focused on encouragement to gather non-functional metrics, and advice how. The most interesting recommendation was to create a virtual team to write functional tests between product engineering, data science, and testing. Other topics were all pertty straightfoward metrics gathering: instrument the app itself or its environment, focus metrics around answering specific questions, iterate which questions you ask of the data.

### Raw Notes
Functional tests are defined in terms of pass/fail. Non-functional tests are about actions, scores, and baselines. Functional tests are checking that the app essentially behaves the same ways. Non-functional testing is important to make sure key product areas continually get faster, more reliable, better--important for scaling the product. These non-functional characteristics are important to users.

Catch non-functional regressions with data. You need data, though you probably have existing sources of data you can tap into. Existing functional tests can log data for non-functional evaluation, end users can also log data.  Beyond your testing team, you'll need skills from your data team and product engineering.

Ultimate goal is to create a (performance) regression safety net. Use this raw data to identify hotspots in the data where you think there can be improvements. Load, search, navigate, etc. Key areas to identify where to add non-functional tests. After looking at these areas, ask how you can better measure them. Team needs to discuss what is a healthy baseline. Track this data as you iterate, every time you publish. If metrics are getting statistically worse for a build/region/time-period/etc, mark a regression! When things look okay in this area, go find new hotspots to improve. Over time, this will make your software faster, more reliable, and a better experience.

One way to collect more data is to instrument the app. Add start/stop bookends into event logging, and regularly emit system statistics. This can be left in the production app to collect data from end users.
Pros:
Get very specific data from app logic and architecture
Easy to get started, just create a new API route
Cons:
Data volume can be very high (you want to analyze too much data)
Privacy issues must be managed (e.g. you cannot get a screenshot or video of the app)

Another approach is to instrument the test environment, by writing a special test harness that capturs video, network, audio, system, code debugger, instrumentation, and more depending on the test platform. (here comes the pitch for this company's tools)
Pros:
Collect data without worrying about impact on app
Cons:
Synthetic testing only, does not include manual testers or end users in the wild

Here's an example of what our report looks like... and then you can use this data to ask questions. Here's some of the questions we've asked - app readiness, api performance, system performance, webpage load, specific function calls, etc. 

Walks through an example of waiting for a web browser page to load. Measure key events in user workflow: open app, navigate to search, submit query, page fully loaded, next step in user flow starts. Automation steps are good at providing regions of interest, but less good at determining the actual user impact. In this case, we use computer vision to determine the actual load time.

Walks through another example of using keyframes between events in a game dialog. Similarly can use visual analysis to match whether the dialog is open or closed, includes things like fading animations. This ML can reduce the flakiness of these animations (hmm...)

Non-functional testing tends to be more successful when structured as part of the project team, rather than as a specific function. Need a member from product engineering, data, and testing. If it's a separate team, it can be very hard to understand the baselines and implement the regression. If you have a team with someone from product engineering, data, and testing, then they will inherently understand the proper scope and targets.

Walks through some simple examples of testing framerate, voice commands, and streaming media. Ends the presentation with links to his company's documentation and learning resources, a jargon term for initial load https://developer.mozilla.org/en-US/docs/Glossary/First_contentful_paint , and a quick pitch for connecting a local device to their testing cloud with a QR code.


## Beyond Test Servers: How 'For Honor' Made Testing a Celebrated Player Experience
Laurent Chouinard, Project Lead Programming at Ubisoft
Audrey Laurent-André, Associate UX Director at Ubisoft
### TL;DR:
Significant player engagement benefit from shifting away from public test servers into a 'testing grounds' feature fully-integrated into the game.
### Summary
For Honor changed its approach to testing away from deploying a test-server to including prototype changes alongside existing content. To help this succeed, they both improved the UX design of how test content was accessed by making it 'part of the game' and also added lasting rewards for participating in regular tests. This reduced the barriers to entry for player-driven testing, and significantly drove player engagement with the test-content. It was a surprise hit, and was attracting players in a similar way as seasonal events. Not only did this eliminate the significant overhead of deploying test servers, it also led to more player types engaging with the test content (not only the enthusiasts).

### Raw Notes
Live balance, why it didn't work, and why created our testing grounds.
Shipped in 2017 during the shift toward live games. Knew we wanted to have a long life. In our case that meant releasing new fighters.

Shipped with 12, and balance was already quite difficult. How are we going to handle balance as we expand this each season.

We often think about winrate, but balance isn't about numbers in a PVP game, it is about how it feels. This changes as players find new ways to use your systems, to evolve and evaluate ourselves we needed a tool to gather more data.

We wanted to avoid a reaction framework, where you push to live and then get feedback where you're on a strict timeline to get a fix out. Wanted to get enough data early, gather insights, and go back to the drawing board if needed.

At that time, test servers was the industry standard. Prepared early on to test our own changes and features. However this went wrong for us.

We had four test servers, and the active players on the servers was around 1.6% and had an overrepresentation of high-skilled players. Also didn't have enough data throughput and timing data for the matchmaking servers.

It was lose-lose. In production we didn't have the data we needed, and it also took extra time to get out to production since we were also deploying to test. Also 99% of players did not see the test content, and had a long response before they saw changes.

Had attendance barriers.  Attendance isn't shared between the servers. Being PC only for test servers reduced things further.

We moved from P2P to dedicated servers, and then we made this work cross-platform, and made a better announcement about . Also had a system to manually reward players. Had around 10x the regular attendance, at around 16% of the players participating. However they weren't doing more than they needed to to get the reward.

How can we do even better? Where do we have fewer entry barriers to players, and less operational cost? The main game itself!

Key for us was to make sure it was a natural part of the player flow, where players choose their activities and game mode.  We do seasonal events, where players can already play timed events.  Would have testing staged as a limited event.  This would tie into our existing progression system.

When a system was ready, we needed a test subject. We still had to keep risky reworks on the back-burner. Integrated this testing grounds feature with an anticipated new character.

We knew this would be better, but not that it would have a 22% attendance from our players.  Working on a one testing ground per season schedule. We know players follow novelty, so the question would be - is this just a fluke? No! Regularly had 24% of active players. Stays steady no matter the content we are moving through it. We didn't expect it to be this consistent over time!

Obvious Benefit: iteration! Steady actvity and attendance give a decent view from across skill levels. Could loop changes through multiple times.

Unforseen Benefit: dialog was more positive and more constructive on social media! Testing grounds were attracting a similar amount of players as the seasonal events, but with less marketing investment. Now when we communicate what we are launching, testing grounds is on our roadmap that we share.

Players don't need to go out of their way to test, there is no separate workflow outside the game for accessing the test content.

Our test servers failed four times for us before we switched. We even toyed with testing grounds before shipping the game. Frankly, we didn't try to ship the testing ground out of fear.

We were afriad of tech complexity.  Worried that there would be changes everywhere! That we'll see unpredictable changes, that QC would be checking the right thing, thought this was impossible.

But why? Where would we need to be changing fighters? Need to make code changes as well as data changes when modifying a fighter. Submit our changes from main to stabilization to submission branches.  Upcomming content is tagged and removed from promoting to another branch.

New live games are kind of new to us, our reflexes are about shipping discs. How would we iterate with sending content to our players? Branches or feature switches? With branches, since it is a different build, then it means the test servers are necessary since it is different. When it looks good, merge back into main branch.  

As soon as we had multiple balancing of heroes in different branches, this became unreasonable! They don't get balanced against one another AND Created a merge hell! Then a second reality check on launching test server. When something gets branched for a while, it needs to be re-stabilized. Plus new QC to do , plus players want their collectables and bling to be copied over ito the test server.  Lots for the live team to manage. Test servers were not easy.

To be efficient at balancing, we need to work in the same branch.  We want to eliminate extra environments to maintain.  We also want console feedback. And to experiment with our other teammeates, because branching would isolate employees from one another.

So why were feature switches "impossible" again? Addressing the fear: tech work amount.  Could add feature switches in parts of the move trees, and data activation conditions can override certain things.

Also had fear of instability. Made a switches sandwich, have good defaults, add local overrides, then create an online override, then change data to enable it in live. Then we integrated this into the liveops workflow, so the live ops team knows what is live and what is upcoming.

Were afraid that removing branches would change how we work.  Turns out that just pressing a single button to try something was way simpler for designers, instead of hopping between downloading branches.

Also, the changes would be exposed earlier to more people, find bad code faster.

Moved to a much more sane workflow of few branches: main, stabilization, live. Broke the branching habit. The success for the fight changes drove this change out to other changes as well, across the game.

Fear of perception, unfinished content in a AAA title?
Clearly communicating this content to players helped. Before playing, it would include a warning about it being beta content. Also added a watermark, in case there are videos online or in bug reports, could clarify where it was from.

This isn't about features at all. It's about how we changed because we were cornered. We thought we had what we needed to go live and make changes. Turns out we were not. Had to change our production mindset, and to change the perception round players complete and incomplete content and not just what has shipped.

We knew what we should have done from the beginning, but we were afriad of it. So instead of addressing the problem, we addressed the fear.


## Creating a Sustainable Volunteer Game Development Community
Justin Chin, Assistant Producer at Gamevil
### TL;DR:
How a producer can help their team.
### Summary
This is a barely-public-sphere spin on common manager/producer wisdom, and doesn't highlight where a community of public volunteers is unique. The speaker highlights what they did and why they did it, but has not distilled any takeaways for us.

### Raw Notes
This is about charity, not about unpaid labor. Games for Love is a 501(3)c charity to ease suffering for those in hospitals, particularly those in third world countries.

Interns international is about people trying to get into the industry. League of Pros is about getting those in the industry to collaborate.

How to Start:
We used free stuff, trello, gsuite, discord, unity and unity team version control, google docs.
Did not want to use P4 or Git, to simplify onboarding.  Unity only allowed free use for 30 days. Would recreate every 30 days.

Problem: people signed up, but didn't do anything. Had around 20% retention, was able to increase to 90%

Solution 1: new onboarding form, with two question types. Contribution: why and how? Direction: where are you now, where do you want to go?
I would review this weekly, have priority referral channels as well.

Discord was good for engagement, for help, immediate and responsive. Almost like a liveops game. As soon as someone starts typing, engage them immediately, let them know there is support. We had team chats and private, role-based categories.

Takeaway: Understand who a preson is who joins a team.

Solution 2: Messaging, marketing material for career fairs. Clear benefits about remote internship, and what is earned through participation.

Solution 3: Create those Benefits. Publish on Steam, apple and android. Partnerships with skillz.com

Solution 4: Building a relationship. This is the primary thing that keeps people, and the only way I know how to do this is with time. This comes down to consistent connecting with team members, individually. They need to know that you care, that you're on they're side. AND you need to prove this.

Regular 1 on 1's, including upwards and laterally.

There's going to be a point where you cannot do more 1:1's, after around 8 this broke down a bit. Need to nurture other leaders, so that you can have someone help manage a team worth of work. Make sure this isn't imbalanced, look at servant leadership principles.

Helped one intern get a design position before visa expiration.

(...stopped taking notes because this is boring)

## Indie Soapbox
(Various)
https://www.gdcvault.com/play/1027343/Independent-Games-Summit-Indie
### TL;DR:
Microtalks: Virtual Pets, Indies in Japan, Dismantling Racism, Work/Life Balance as an RPG, Serverless Social Networks, Better Representation, Dynamics and Pacing, "Indie" Describes Developers not Games, and Keeping a Community Alive.
### Summary
Unsurprisingly the content was a grab bag value, but some of it was great. Since the talks are each condensed into five minutes, the presenters talk fast and rarely summarize takeaways. Standout talks are from Latoya Peterson on applying game design to systems of racism and Andi McClure on prototyping a social network without servers.

### Raw Notes

Nathalie Lawhead
Makes artgames, wierd things on computers are what I live for. I'm going to talk about desktop pets, and why you should make them. Our desktops are spaces we spend a lot of time in. Spend as much time there as anywhere else in the real world, however these spaces are often invisible to us. Interesting how obsolete they feel after spending time in them. In the early 2000's they were viewed more as a home, can be seen from some of the leftover naming conventions. Back then there were thriving theming communities around theming things like the desktop or media player skins. Used to be normal to do with the software you use. Today it feels more like you are renting your software from Microsoft or Apple. There are silly old apps that made a simulated pet (oh god clippy would be included). As game designers we can learn a lot from desktop pets, this era of computing, and what we can expect of a computer. Fluid design, modern ui, and banner ads are more corporate than these spaces used to hold. Games can exist outside the confines of their packaging. There's unexplored space which we are taking for granted. Figure out what it means for yourself for something to exist here.

Alvin Phu
Indie/Doujin games in Japan. I run a indie publisher in Tokyo, Hanaji games. Previously founded Tokyo Indies which (before the pandemic) was the largest meetup of indies in Japan. Japanese culture that that western scene don't understand: Dojin is an umbrella term for self-published creative works. Around 2011 everything was distributed this way, either distributing physical copies from a print store, or a free executable online. Name Tokyo Indies is actually a bit confusing in Japan because some Dojin creators think it may not apply to them. There is a *lot* of adult and parody doujin, but it is not all of the community. On the other hand the term Doujin is often misunderstood in the west as being specific to pornographic content. Touhou and Cave Story are likely the largest names in the Doujin game space. Both came out within two years of one another, one was considered 'doujin' and the other 'indie' ...is it because one features more anime portraits? Really, more or less, these are the same term! Still been difficult to bridge these terms and communities, for instance almost no one in the Doujin scene attends GDC. Sometimes feel awkward as there are also other indie scenes deserving of attention.

Latoya Peterson
Spent the first decade of my career working on digital media and anti-racism, much of that in videogames. Games revolve around system design, and racism is just a system. In games this is an intentional system that is fine-tuned to make players successful in navigating these worlds. Where the industry is less successful is in inclusion, especially in race: around 3% of survey respondents have been black. Summer 2020 was a turning point in civic action and uprising, including police brutality and the inequality that black citizens face. Games community was mixed, though this is not specific to games and seen in other industires. Have seen all kinds of pledges from companies to make changes and apply money, however tech companies appear to have simply used their standard playbooks. Puzzling that gaming would not succeed, as games have a unique set of tools to dismantle flawed frameworks. Can clearly map how they work, and where certain players are being advantaged/disadvantaged. We can apply a systems design framework to other racist systems, such as the US tax code. This system was not designed with race in mind, and does not collect data about race. This can camoflauge the data on inequality. NFL was relying on inequality in the medical system where fewer insurance payments were paid out for repeated concussive injuries because other races were assumed to have a lower cognitive baseline, as it is harder to quality for an injury-related cognifive defecit. After the tax and NFL landscape were mapped and understood, it was easy to implement a change. Why aren't these gaps being applied in the field, if they are normal for game design? (Keep asking yourself this question)

Alejandro Quan-Madrid
Teacher and game designer based out of LA. How to make games sustainably using "healing spells'. I'm a bit of a generalist, tons of hats, sounds fun and it is... but it's also exhausting, brain can turn to much, decision fatigue. Feels impossible to work on something, make a choice. For game designers we need this energy to work. Can view this like MP in an RPG game. Picking, choosing what you're going to wear or eat, resisting temptations, fighting about whether to exercise, judging social situations - All of this uses this energy. Oh no, I have a decision fatigue status ailment. "what do you want to eat/watch/etc." responded with "I dunno" ...try to recognize when you are done with the work session, look at an easy task and gauge whether it feels hard. If you keep going, you may mis-judge whether you can keep going. What you should be doing is "cast healing spells" perhaps you can eat something to give you a short burst of energy, but this is short-term. Clean your room, groom yourself, cook and eat something, do that fun thing you were putting off because "you're supposed to be working" to recoup. Avoid just doing the easiest thing like watching TV. Sleep at least 7 hours, and make leisure time. Some tasks are like a poison status effect, the "I still have to do that" task, be sure to have a way to heal this. Some tasks cause dread, scan through certain tasks to understand them better. Start healing as early as possible in the day to get the most out of your work. Recognize when you are done as early as possible, and start healing again.

Andi McClure
Here to talk about decentralized social networking. A twitter clone with no central server, just browser tabs talking to other browser tabs. Why am I doing this? Three separate cloud providers I was depending on shut down. My clone is similar to Mastadon, instead of talking to one server everyone talks to different servers, like an e-mail address. I think this is great since social networks are basically good ideas, but the people who run existing platforms have made bad decisions. Different servers let server runners make better decisions. Problem with Mastodon is that occasionally a server will shut down and everything on it will be lost. Mastodon is federated, somewhere between centralized and decentralized. What I want to do is to make a decentralized twitter-like. This has been attempted before (Freenet, diaspora, hypercore/dat/seaker) but has failed because everyone has to download and configure software. I want to set this up so everything runs in a browser tab, which is what most users want these days. Through distributed computing, there are two common data structures, ledgers and distributed hash tables. Change a long block of content and look it up with a shorter number that mathematically represents the entire thing. Going to use Merkle Tree Accumulators, a type of ledger, to cut up content into different pieces and then store different pieces in a post-history. Going to take the number at the top, and then store it in a Distributed Hash Tree, to get the ledger for a person. When the merkle tree changes, you don't have to redownload the entire thing, only have to grab what changed. Has a big problem in key rotation, have a solution but no time to explain it. Going from one browser tab to another is simple, can use libp2p. I have done this and have a proof of concept, but the parts aren't hooked up yet.

Shawn Alexander Allen
This is "Representation Matters" but not in the way you are probably thinking about. Just released a game, help run the Game Devs of Color Expo. This talk is basically talking about my last decade in games, and distilling it. Publishers: I remember years ago that "you don't need a publisher" was wisdom, there's a great power imbalance about placement in ad banners. "Success" doesn't always mean success, I have been told that platform buyouts (Epic buying your game to distribute) are good but be careful to not give your game out (to Epic) for free even if they give it out for free to customers. Marginalized devs need a place to thrive, and a throughline is that underrepresented devs have many issues around scrutiny or other practices based on racism (starts name-dropping existing company initiatves). Producers are extremely helpful, had to remove some of my preciousness from my game, and I think we need more freelance producers. While box art is showing more diverse characters, not all the representation is positive. Consult with the groups you are trying to portray, listen to many voices.

Sarah Scialli
Last year I noted that I often find myself pulled between games, film, and theater. I mentioned that I find these three areas of my life very close to one another. While they are codified as different disciplines, a lot of learning in one applies to another. Reversing the combination from choreography is very similar to debugging. In the past year of change and transition, I've been astounded to how the film industry has started adopting lessons from the game world. The theatre world has also moved closer to the film and games world. I am going to reflect futher, and add a few more ideas. Dynamics, a loudness or softness, allow us to apply our own emotional choices to song, dance, and other performances - the point is to keep it changing. Film intensity graphics are similar to dynamics of an individual. Of course this can be applied to games, not only in plot and framing but also in resources, complexity, and other layers of meaning. In film editing we pay atention to where the eye will be, and want to draw it around the frame. Can introduce a new color or intensity just as the eyes are adjusting, which will slightly dissorient the audience - do this on purpose, similar to dynamics.

Sun Park
I am running a game studio since 2009. Also organize some events in Seoul, South Korea. Going to start with an old and tired question, "what is an indie game" and instead of actually talking about this, I'm going to propose that nobody use this term anymore. If someone "is making an indie game" what are they talking about? Is it about a small team, or with a low price? Is there a difference between "platformer game" and "indie platformer game"? Perhaps you can feel some difference, but a lot of people will think of different things. "Indie" seems to exist as declaring something as a creator, not about the game. Not a genre, not a style, doesn't necessarily relate to quality of game. "Indie-game developer" vs "indie game-developer" we are talking about the latter. Don't care about any thought about "indie game" it is useless, thera are only two things in your game dev life: you and your game. Focus on how to make your game better for yourself.

Rachel Sala
Worked in games, also manage an LA-based indie game dev collective Glitch City. Going to talk about how we adapted the last couple years (to the pandemic). Techincally started back in 2013, but before that there were weekly meetups in coffee shops. This relieved solitude of independent development. Glitch City is a more-permanent version of these meetups, have an office in Culver City. This was before my time. I first heard about them in 2014. All of my development was done in my room, a few feet from my bed. Thought about moving to Bay Area  since I didn't know any local developers. Started attending and loved it, driving after work every day to spend a few hours there. Started working indie full time in 2016, shortly after joined leadership at Glitch City. Quite a few successful games have come out of this collective. Have weekly and yearly events. Being part of this community has changed my life for the better. I get to go in and be surrounded by like-minded creative people. Hasn't always been easy, money is always a problem - LA is expensive and indie devs don't always make much. Rely on a lot of volunteer help, run a patreon, charge small amounts for community events. Need people to help coordinate administrative duties. All possible because everyone is a volunteer. People are willing to put a lot of energy and time into it. Found it useful to do weekly meetups as mangers, to resolve any problems. Last year we were stuck with a lease we couldn't benefit from, community support mostly dried up, had to move our events online. Made a community-oriented Discord server, people can join into a channel to interact. Moved social events into cheap or free multiplayer games. Art nights moved online, instead of costumed figure drawing started doing shared canvas and topic-based exercises. Movie night was easy to move online with Discord. Also ran a game jam online, random groups, great way to spend time with people we hadn't seen in a long time. Released these games as a fundraiser bundle, with was successful. Made it through, hope to stick around for a long time.
